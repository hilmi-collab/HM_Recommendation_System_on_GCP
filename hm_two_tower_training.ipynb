{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# @title ‚öôÔ∏è Workshop Configuration\n",
        "# @markdown Please enter your Project ID.\n",
        "\n",
        "import os\n",
        "\n",
        "# @markdown ### ‚òÅÔ∏è Project Settings\n",
        "PROJECT_ID = \"ace-team-hilmi-service\" # @param {type:\"string\"}\n",
        "REGION = \"us-central1\" # @param {type:\"string\"}\n",
        "\n",
        "# 1. PUBLIC DATA BUCKET (Read-Only)\n",
        "# Raw data (CSV) will be read from here.\n",
        "DATA_BUCKET_NAME = \"hm-recommendation-workshop\"\n",
        "DATA_GCS_PATH = f\"gs://{DATA_BUCKET_NAME}\"\n",
        "\n",
        "# 2. PRIVATE WORK BUCKET (Write)\n",
        "# Trained models will be saved here.\n",
        "WORK_BUCKET_NAME = f\"hm-workshop-{PROJECT_ID}\"\n",
        "WORK_GCS_PATH = f\"gs://{WORK_BUCKET_NAME}\"\n",
        "\n",
        "# @markdown ### üöÄ Model Hyperparameters\n",
        "EMBEDDING_DIM = 64 # @param {type:\"integer\"}\n",
        "LEARNING_RATE = 0.1 # @param {type:\"number\"}\n",
        "EPOCHS = 5 # @param {type:\"slider\", min:1, max:10, step:1}\n",
        "\n",
        "# Setup Environment\n",
        "os.environ[\"GCLOUD_PROJECT\"] = PROJECT_ID\n",
        "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n",
        "\n",
        "print(f\"‚úÖ Config Set:\")\n",
        "print(f\"   üì• Reading Data from: {DATA_GCS_PATH}\")\n",
        "print(f\"   üíæ Saving Models to:  {WORK_GCS_PATH}/models/two-tower-model\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# @title üì• Step 1: Install Libraries\n",
        "!pip install -q tensorflow-recommenders --no-deps\n",
        "!pip install -q --upgrade tensorflow-datasets\n",
        "!pip install -q \"scann[tf]\" tensorflow-recommenders tensorflow-datasets\n",
        "print(\"‚úÖ Installation Complete.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# @title üíæ Step 2: Load Data from Public Bucket\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_recommenders as tfrs\n",
        "from typing import Dict, Text, List\n",
        "from tensorflow.keras.layers import StringLookup, Embedding, Dense\n",
        "\n",
        "print(f\"TensorFlow Version: {tf.__version__}\")\n",
        "\n",
        "# Paths (Reading from DATA_GCS_PATH)\n",
        "ARTICLES_PATH = os.path.join(DATA_GCS_PATH, 'articles.csv')\n",
        "CUSTOMERS_PATH = os.path.join(DATA_GCS_PATH, 'customers.csv')\n",
        "TRANSACTIONS_PATH = os.path.join(DATA_GCS_PATH, 'transactions.csv')\n",
        "\n",
        "# --- 1. Load Articles ---\n",
        "print(f\"Loading Articles from: {ARTICLES_PATH}\")\n",
        "ARTICLE_FEATURES = ['article_id', 'product_type_name', 'product_group_name', 'colour_group_name', 'department_name']\n",
        "articles_df = pd.read_csv(ARTICLES_PATH, usecols=ARTICLE_FEATURES, dtype={'article_id': str})\n",
        "for col in ARTICLE_FEATURES:\n",
        "    if col != 'article_id':\n",
        "        articles_df[col] = articles_df[col].fillna('Unknown')\n",
        "\n",
        "# --- 2. Load Customers ---\n",
        "print(f\"Loading Customers from: {CUSTOMERS_PATH}\")\n",
        "customers_df = pd.read_csv(CUSTOMERS_PATH, usecols=['customer_id', 'age'])\n",
        "age_bins = [0, 19, 25, 29, 35, 39, 45, 49, 59, 69, 100]\n",
        "age_labels = ['0-19', '20-25', '26-29', '30-35', '36-39', '40-45', '46-49', '50-59', '60-69', '70+']\n",
        "customers_df['age_bin'] = pd.cut(customers_df['age'], bins=age_bins, labels=age_labels, right=False)\n",
        "customers_df['age_bin'] = customers_df['age_bin'].cat.add_categories('Unknown').fillna('Unknown').astype(str)\n",
        "\n",
        "# --- 3. Load Transactions ---\n",
        "print(f\"Loading Transactions from: {TRANSACTIONS_PATH}\")\n",
        "transactions_df = pd.read_csv(TRANSACTIONS_PATH, parse_dates=['t_dat'], dtype={'article_id': str})\n",
        "\n",
        "val_start_date = pd.to_datetime('2020-09-09')\n",
        "train_start_date = val_start_date - pd.DateOffset(years=1)\n",
        "train_df = transactions_df[\n",
        "    (transactions_df['t_dat'] < val_start_date) &\n",
        "    (transactions_df['t_dat'] >= train_start_date)\n",
        "].copy()\n",
        "\n",
        "train_df['month_of_year'] = train_df['t_dat'].dt.month.astype(str)\n",
        "train_df['week_of_month'] = ((train_df['t_dat'].dt.day - 1) // 7 + 1).astype(str)\n",
        "interactions_df = train_df[['customer_id', 'article_id', 'month_of_year', 'week_of_month']]\n",
        "\n",
        "print(f\"‚úÖ Training dataset ready: {len(interactions_df)} rows.\")\n",
        "del transactions_df, train_df"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# @title üîß Step 3: Preprocessing\n",
        "customer_ids = customers_df['customer_id'].unique()\n",
        "article_ids = articles_df['article_id'].unique()\n",
        "age_groups = customers_df['age_bin'].unique()\n",
        "months = [str(i) for i in range(1, 13)]\n",
        "weeks = [str(i) for i in range(1, 6)]\n",
        "\n",
        "print(\"Creating Lookup Tables...\")\n",
        "cust_age_table = tf.lookup.StaticHashTable(\n",
        "    tf.lookup.KeyValueTensorInitializer(customers_df['customer_id'], customers_df['age_bin']),\n",
        "    default_value='Unknown'\n",
        ")\n",
        "\n",
        "article_tables = {}\n",
        "feature_cols = ['product_type_name', 'product_group_name', 'colour_group_name', 'department_name']\n",
        "for col in feature_cols:\n",
        "    article_tables[col] = tf.lookup.StaticHashTable(\n",
        "        tf.lookup.KeyValueTensorInitializer(articles_df['article_id'], articles_df[col]),\n",
        "        default_value='Unknown'\n",
        "    )\n",
        "\n",
        "articles_ds = tf.data.Dataset.from_tensor_slices(dict(articles_df))\n",
        "interactions_ds = tf.data.Dataset.from_tensor_slices(dict(interactions_df))\n",
        "\n",
        "def add_features(features):\n",
        "    features['age_bin'] = cust_age_table.lookup(features['customer_id'])\n",
        "    for col in feature_cols:\n",
        "        features[col] = article_tables[col].lookup(features['article_id'])\n",
        "    return features\n",
        "\n",
        "interactions_ds = interactions_ds.map(add_features, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "print(\"‚úÖ Lookup tables created.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# @title üß† Step 4: Define Two-Tower Model\n",
        "class UserModel(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.customer_id_lookup = StringLookup(vocabulary=customer_ids, mask_token=None)\n",
        "        self.customer_id_emb = Embedding(len(customer_ids) + 1, EMBEDDING_DIM)\n",
        "        self.age_bin_lookup = StringLookup(vocabulary=age_groups, mask_token=None)\n",
        "        self.age_bin_emb = Embedding(len(age_groups) + 1, EMBEDDING_DIM // 4)\n",
        "        self.month_lookup = StringLookup(vocabulary=months, mask_token=None)\n",
        "        self.month_emb = Embedding(len(months) + 1, EMBEDDING_DIM // 4)\n",
        "        self.week_lookup = StringLookup(vocabulary=weeks, mask_token=None)\n",
        "        self.week_emb = Embedding(len(weeks) + 1, EMBEDDING_DIM // 4)\n",
        "        self.projection = Dense(EMBEDDING_DIM)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = tf.concat([\n",
        "            self.customer_id_emb(self.customer_id_lookup(inputs['customer_id'])),\n",
        "            self.age_bin_emb(self.age_bin_lookup(inputs['age_bin'])),\n",
        "            self.month_emb(self.month_lookup(inputs['month_of_year'])),\n",
        "            self.week_emb(self.week_lookup(inputs['week_of_month'])),\n",
        "        ], axis=1)\n",
        "        return self.projection(x)\n",
        "\n",
        "class ItemModel(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.article_id_lookup = StringLookup(vocabulary=article_ids, mask_token=None)\n",
        "        self.article_id_emb = Embedding(len(article_ids) + 1, EMBEDDING_DIM)\n",
        "        self.lookups = {}\n",
        "        self.embeddings = {}\n",
        "        for col in feature_cols:\n",
        "            vocab = articles_df[col].unique()\n",
        "            self.lookups[col] = StringLookup(vocabulary=vocab, mask_token=None)\n",
        "            self.embeddings[col] = Embedding(len(vocab) + 1, EMBEDDING_DIM // 4)\n",
        "        self.projection = Dense(EMBEDDING_DIM)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        embs = [self.article_id_emb(self.article_id_lookup(inputs['article_id']))]\n",
        "        for col in feature_cols:\n",
        "            embs.append(self.embeddings[col](self.lookups[col](inputs[col])))\n",
        "        x = tf.concat(embs, axis=1)\n",
        "        return self.projection(x)\n",
        "\n",
        "class HMRModel(tfrs.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.user_model = UserModel()\n",
        "        self.item_model = ItemModel()\n",
        "        self.task = tfrs.tasks.Retrieval()\n",
        "\n",
        "    def compute_loss(self, features, training=False):\n",
        "        user_embeddings = self.user_model(features)\n",
        "        item_embeddings = self.item_model(features)\n",
        "        return self.task(user_embeddings, item_embeddings)\n",
        "\n",
        "print(\"‚úÖ Model architecture defined.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# @title üèãÔ∏è Step 5: Train the Model\n",
        "cached_train = interactions_ds.shuffle(100_000).batch(16384).cache().prefetch(tf.data.AUTOTUNE)\n",
        "model = HMRModel()\n",
        "model.compile(optimizer=tf.keras.optimizers.Adagrad(LEARNING_RATE))\n",
        "print(f\"Starting training for {EPOCHS} epochs...\")\n",
        "history = model.fit(cached_train, epochs=EPOCHS)\n",
        "print(\"‚úÖ Training finished.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# @title üîç Step 6: Build ScaNN Index\n",
        "scann_index = tfrs.layers.factorized_top_k.ScaNN(\n",
        "    model.user_model,\n",
        "    num_reordering_candidates=500,\n",
        "    num_leaves=1000,\n",
        "    num_leaves_to_search=30,\n",
        "    k=50\n",
        ")\n",
        "candidate_dataset = articles_ds.batch(2048).map(lambda x: (x[\"article_id\"], model.item_model(x)))\n",
        "scann_index.index_from_dataset(candidate_dataset)\n",
        "\n",
        "sample_query = {\n",
        "    \"customer_id\": tf.constant([customer_ids[0]]),\n",
        "    \"age_bin\": tf.constant([age_groups[0]]),\n",
        "    \"month_of_year\": tf.constant([\"9\"]),\n",
        "    \"week_of_month\": tf.constant([\"2\"])\n",
        "}\n",
        "_ = scann_index(sample_query)\n",
        "print(\"‚úÖ ScaNN index built successfully.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# @title üíæ Step 7: Save Model to User Bucket\n",
        "class ServingModel(tf.keras.Model):\n",
        "    def __init__(self, index_layer):\n",
        "        super().__init__()\n",
        "        self.index_layer = index_layer\n",
        "\n",
        "    @tf.function(input_signature=[{\n",
        "        \"customer_id\": tf.TensorSpec(shape=(None,), dtype=tf.string),\n",
        "        \"age_bin\": tf.TensorSpec(shape=(None,), dtype=tf.string),\n",
        "        \"month_of_year\": tf.TensorSpec(shape=(None,), dtype=tf.string),\n",
        "        \"week_of_month\": tf.TensorSpec(shape=(None,), dtype=tf.string)\n",
        "    }])\n",
        "    def call(self, features):\n",
        "        return self.index_layer(features)\n",
        "\n",
        "serving_model = ServingModel(scann_index)\n",
        "_ = serving_model(sample_query)\n",
        "\n",
        "# SAVING THE MODEL TO PRIVATE WORK BUCKET\n",
        "MODEL_SAVE_PATH = os.path.join(WORK_GCS_PATH, 'models/two-tower-model')\n",
        "print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
        "tf.saved_model.save(serving_model, MODEL_SAVE_PATH)\n",
        "print(\"‚úÖ Model saved successfully.\")"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}