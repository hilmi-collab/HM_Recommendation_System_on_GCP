{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afe5911",
   "metadata": {
    "cellView": "form",
    "id": "config_cell"
   },
   "outputs": [],
   "source": [
    "# @title ‚öôÔ∏è Workshop Configuration & Setup\n",
    "# @markdown Please enter your project details and training parameters below.\n",
    "# @markdown ---\n",
    "\n",
    "import os\n",
    "\n",
    "# @markdown ### ‚òÅÔ∏è Cloud Project Settings\n",
    "PROJECT_ID = \"your-project-id-here\" # @param {type:\"string\"}\n",
    "BUCKET_NAME = \"hm-recommendation-workshop\" # @param {type:\"string\"}\n",
    "REGION = \"us-central1\" # @param {type:\"string\"}\n",
    "\n",
    "# @markdown ### üöÄ Model Hyperparameters\n",
    "EMBEDDING_DIM = 64 # @param {type:\"integer\"}\n",
    "LEARNING_RATE = 0.1 # @param {type:\"number\"}\n",
    "EPOCHS = 5 # @param {type:\"slider\", min:1, max:10, step:1}\n",
    "\n",
    "# @markdown ### üì¶ Data Paths (Relative to Bucket)\n",
    "# @markdown Do not change these unless your bucket structure is different.\n",
    "ARTICLES_FILE = \"articles.csv\" # @param {type:\"string\"}\n",
    "CUSTOMERS_FILE = \"customers.csv\" # @param {type:\"string\"}\n",
    "TRANSACTIONS_FILE = \"transactions.csv\" # @param {type:\"string\"}\n",
    "\n",
    "# Setup Environment Variables\n",
    "os.environ[\"GCLOUD_PROJECT\"] = PROJECT_ID\n",
    "# Important: Use legacy Keras behavior for TF 2.x compatibility with ScaNN\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\" \n",
    "\n",
    "GCS_BASE_PATH = f\"gs://{BUCKET_NAME}\"\n",
    "\n",
    "print(f\"‚úÖ Configuration set for Project: {PROJECT_ID}\")\n",
    "print(f\"üìÇ Data Source: {GCS_BASE_PATH}\")\n",
    "print(f\"weights will be saved to: {GCS_BASE_PATH}/models/two-tower-model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e153cdfe",
   "metadata": {
    "cellView": "form",
    "id": "install_cell"
   },
   "outputs": [],
   "source": [
    "# @title üì• Step 1: Install Libraries\n",
    "# @markdown Installing TensorFlow Recommenders, ScaNN, and Datasets.\n",
    "# @markdown This may take 1-2 minutes.\n",
    "\n",
    "import sys\n",
    "\n",
    "# Install TF-compatible version of ScaNN and other dependencies\n",
    "!pip install -q tensorflow-recommenders --no-deps\n",
    "!pip install -q --upgrade tensorflow-datasets\n",
    "!pip install -q \"scann[tf]\" tensorflow-recommenders tensorflow-datasets\n",
    "\n",
    "print(\"‚úÖ Installation Complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c95f02",
   "metadata": {
    "cellView": "form",
    "id": "data_load_cell"
   },
   "outputs": [],
   "source": [
    "# @title üíæ Step 2: Load Data from Cloud Storage\n",
    "# @markdown Reading CSV files directly from your GCS Bucket using the paths defined in configuration.\n",
    "\n",
    "import os\n",
    "import pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "from typing import Dict, Text, List\n",
    "from tensorflow.keras.layers import StringLookup, Embedding, Dense\n",
    "\n",
    "# GPU Check\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"üöÄ GPU Active: {gpus[0].name}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Running on CPU (Training might be slower)\")\n",
    "\n",
    "# Paths\n",
    "ARTICLES_PATH = os.path.join(GCS_BASE_PATH, ARTICLES_FILE)\n",
    "CUSTOMERS_PATH = os.path.join(GCS_BASE_PATH, CUSTOMERS_FILE)\n",
    "TRANSACTIONS_PATH = os.path.join(GCS_BASE_PATH, TRANSACTIONS_FILE)\n",
    "\n",
    "# --- 1. Load Articles ---\n",
    "print(f\"Loading Articles from: {ARTICLES_PATH}\")\n",
    "ARTICLE_FEATURES = ['article_id', 'product_type_name', 'product_group_name', 'colour_group_name', 'department_name']\n",
    "articles_df = pd.read_csv(ARTICLES_PATH, usecols=ARTICLE_FEATURES, dtype={'article_id': str})\n",
    "for col in ARTICLE_FEATURES:\n",
    "    if col != 'article_id':\n",
    "        articles_df[col] = articles_df[col].fillna('Unknown')\n",
    "\n",
    "# --- 2. Load Customers ---\n",
    "print(f\"Loading Customers from: {CUSTOMERS_PATH}\")\n",
    "customers_df = pd.read_csv(CUSTOMERS_PATH, usecols=['customer_id', 'age'])\n",
    "age_bins = [0, 19, 25, 29, 35, 39, 45, 49, 59, 69, 100]\n",
    "age_labels = ['0-19', '20-25', '26-29', '30-35', '36-39', '40-45', '46-49', '50-59', '60-69', '70+']\n",
    "customers_df['age_bin'] = pd.cut(customers_df['age'], bins=age_bins, labels=age_labels, right=False)\n",
    "customers_df['age_bin'] = customers_df['age_bin'].cat.add_categories('Unknown').fillna('Unknown').astype(str)\n",
    "\n",
    "# --- 3. Load Transactions ---\n",
    "print(f\"Loading Transactions from: {TRANSACTIONS_PATH}\")\n",
    "transactions_df = pd.read_csv(TRANSACTIONS_PATH, parse_dates=['t_dat'], dtype={'article_id': str})\n",
    "\n",
    "# Filter last year data\n",
    "val_start_date = pd.to_datetime('2020-09-09')\n",
    "train_start_date = val_start_date - pd.DateOffset(years=1)\n",
    "train_df = transactions_df[\n",
    "    (transactions_df['t_dat'] < val_start_date) & \n",
    "    (transactions_df['t_dat'] >= train_start_date)\n",
    "].copy()\n",
    "\n",
    "# Feature Engineering\n",
    "train_df['month_of_year'] = train_df['t_dat'].dt.month.astype(str)\n",
    "train_df['week_of_month'] = ((train_df['t_dat'].dt.day - 1) // 7 + 1).astype(str)\n",
    "interactions_df = train_df[['customer_id', 'article_id', 'month_of_year', 'week_of_month']]\n",
    "\n",
    "print(f\"‚úÖ Training dataset ready: {len(interactions_df)} rows.\")\n",
    "\n",
    "# Clean up RAM\n",
    "del transactions_df, train_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312bb7b8",
   "metadata": {
    "cellView": "form",
    "id": "prep_cell"
   },
   "outputs": [],
   "source": [
    "# @title üîß Step 3: Preprocessing & Lookup Tables\n",
    "# @markdown Creating string lookup tables for features (Age, Product Group, Department, etc.)\n",
    "\n",
    "customer_ids = customers_df['customer_id'].unique()\n",
    "article_ids = articles_df['article_id'].unique()\n",
    "age_groups = customers_df['age_bin'].unique()\n",
    "months = [str(i) for i in range(1, 13)]\n",
    "weeks = [str(i) for i in range(1, 6)]\n",
    "\n",
    "print(\"Creating Lookup Tables...\")\n",
    "cust_age_table = tf.lookup.StaticHashTable(\n",
    "    tf.lookup.KeyValueTensorInitializer(customers_df['customer_id'], customers_df['age_bin']),\n",
    "    default_value='Unknown'\n",
    ")\n",
    "\n",
    "article_tables = {}\n",
    "feature_cols = ['product_type_name', 'product_group_name', 'colour_group_name', 'department_name']\n",
    "for col in feature_cols:\n",
    "    article_tables[col] = tf.lookup.StaticHashTable(\n",
    "        tf.lookup.KeyValueTensorInitializer(articles_df['article_id'], articles_df[col]),\n",
    "        default_value='Unknown'\n",
    "    )\n",
    "\n",
    "# Create Datasets\n",
    "articles_ds = tf.data.Dataset.from_tensor_slices(dict(articles_df))\n",
    "interactions_ds = tf.data.Dataset.from_tensor_slices(dict(interactions_df))\n",
    "\n",
    "def add_features(features):\n",
    "    features['age_bin'] = cust_age_table.lookup(features['customer_id'])\n",
    "    for col in feature_cols:\n",
    "        features[col] = article_tables[col].lookup(features['article_id'])\n",
    "    return features\n",
    "\n",
    "interactions_ds = interactions_ds.map(add_features, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "print(\"‚úÖ Lookup tables and pipelines created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2992fe7d",
   "metadata": {
    "cellView": "form",
    "id": "model_def_cell"
   },
   "outputs": [],
   "source": [
    "# @title üß† Step 4: Define Two-Tower Model\n",
    "# @markdown Defining User Tower and Item Tower architectures using TensorFlow Recommenders.\n",
    "\n",
    "class UserModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.customer_id_lookup = StringLookup(vocabulary=customer_ids, mask_token=None)\n",
    "        self.customer_id_emb = Embedding(len(customer_ids) + 1, EMBEDDING_DIM)\n",
    "        \n",
    "        self.age_bin_lookup = StringLookup(vocabulary=age_groups, mask_token=None)\n",
    "        self.age_bin_emb = Embedding(len(age_groups) + 1, EMBEDDING_DIM // 4)\n",
    "        \n",
    "        self.month_lookup = StringLookup(vocabulary=months, mask_token=None)\n",
    "        self.month_emb = Embedding(len(months) + 1, EMBEDDING_DIM // 4)\n",
    "        \n",
    "        self.week_lookup = StringLookup(vocabulary=weeks, mask_token=None)\n",
    "        self.week_emb = Embedding(len(weeks) + 1, EMBEDDING_DIM // 4)\n",
    "        self.projection = Dense(EMBEDDING_DIM)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = tf.concat([\n",
    "            self.customer_id_emb(self.customer_id_lookup(inputs['customer_id'])),\n",
    "            self.age_bin_emb(self.age_bin_lookup(inputs['age_bin'])),\n",
    "            self.month_emb(self.month_lookup(inputs['month_of_year'])),\n",
    "            self.week_emb(self.week_lookup(inputs['week_of_month'])),\n",
    "        ], axis=1)\n",
    "        return self.projection(x)\n",
    "\n",
    "class ItemModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.article_id_lookup = StringLookup(vocabulary=article_ids, mask_token=None)\n",
    "        self.article_id_emb = Embedding(len(article_ids) + 1, EMBEDDING_DIM)\n",
    "        \n",
    "        self.lookups = {}\n",
    "        self.embeddings = {}\n",
    "        for col in feature_cols:\n",
    "            vocab = articles_df[col].unique()\n",
    "            self.lookups[col] = StringLookup(vocabulary=vocab, mask_token=None)\n",
    "            self.embeddings[col] = Embedding(len(vocab) + 1, EMBEDDING_DIM // 4)\n",
    "        self.projection = Dense(EMBEDDING_DIM)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        embs = [self.article_id_emb(self.article_id_lookup(inputs['article_id']))]\n",
    "        for col in feature_cols:\n",
    "            embs.append(self.embeddings[col](self.lookups[col](inputs[col])))\n",
    "        x = tf.concat(embs, axis=1)\n",
    "        return self.projection(x)\n",
    "\n",
    "class HMRModel(tfrs.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.user_model = UserModel()\n",
    "        self.item_model = ItemModel()\n",
    "        self.task = tfrs.tasks.Retrieval()\n",
    "\n",
    "    def compute_loss(self, features, training=False):\n",
    "        user_embeddings = self.user_model(features)\n",
    "        item_embeddings = self.item_model(features)\n",
    "        return self.task(user_embeddings, item_embeddings)\n",
    "\n",
    "print(\"‚úÖ Model architecture defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedd4f5c",
   "metadata": {
    "cellView": "form",
    "id": "train_cell"
   },
   "outputs": [],
   "source": [
    "# @title üèãÔ∏è Step 5: Train the Model\n",
    "# @markdown Training the Two-Tower model with Adagrad optimizer.\n",
    "# @markdown *Note: This uses the Epochs and Learning Rate defined in Step 1.*\n",
    "\n",
    "# Cache data in RAM\n",
    "cached_train = interactions_ds.shuffle(100_000).batch(16384).cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "model = HMRModel()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(LEARNING_RATE))\n",
    "\n",
    "print(f\"Starting training for {EPOCHS} epochs...\")\n",
    "history = model.fit(cached_train, epochs=EPOCHS)\n",
    "print(\"‚úÖ Training finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e9c6c5",
   "metadata": {
    "cellView": "form",
    "id": "scann_cell"
   },
   "outputs": [],
   "source": [
    "# @title üîç Step 6: Build ScaNN Index\n",
    "# @markdown Building Approximate Nearest Neighbor index for fast retrieval.\n",
    "\n",
    "print(\"Building ScaNN index...\")\n",
    "scann_index = tfrs.layers.factorized_top_k.ScaNN(\n",
    "    model.user_model,\n",
    "    num_reordering_candidates=500,\n",
    "    num_leaves=1000,\n",
    "    num_leaves_to_search=30,\n",
    "    k=50\n",
    ")\n",
    "\n",
    "# Index all items\n",
    "candidate_dataset = articles_ds.batch(2048).map(lambda x: (x[\"article_id\"], model.item_model(x)))\n",
    "scann_index.index_from_dataset(candidate_dataset)\n",
    "\n",
    "# Build check\n",
    "sample_query = {\n",
    "    \"customer_id\": tf.constant([customer_ids[0]]),\n",
    "    \"age_bin\": tf.constant([age_groups[0]]),\n",
    "    \"month_of_year\": tf.constant([\"9\"]),\n",
    "    \"week_of_month\": tf.constant([\"2\"])\n",
    "}\n",
    "_ = scann_index(sample_query)\n",
    "print(\"‚úÖ ScaNN index built successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29042cbf",
   "metadata": {
    "cellView": "form",
    "id": "save_cell"
   },
   "outputs": [],
   "source": [
    "# @title üíæ Step 7: Save Model to GCS\n",
    "# @markdown Saving the serving-ready model to Google Cloud Storage.\n",
    "\n",
    "class ServingModel(tf.keras.Model):\n",
    "    def __init__(self, index_layer):\n",
    "        super().__init__()\n",
    "        self.index_layer = index_layer\n",
    "\n",
    "    @tf.function(input_signature=[{\n",
    "        \"customer_id\": tf.TensorSpec(shape=(None,), dtype=tf.string),\n",
    "        \"age_bin\": tf.TensorSpec(shape=(None,), dtype=tf.string),\n",
    "        \"month_of_year\": tf.TensorSpec(shape=(None,), dtype=tf.string),\n",
    "        \"week_of_month\": tf.TensorSpec(shape=(None,), dtype=tf.string)\n",
    "    }])\n",
    "    def call(self, features):\n",
    "        return self.index_layer(features)\n",
    "\n",
    "serving_model = ServingModel(scann_index)\n",
    "_ = serving_model(sample_query) # Final build check\n",
    "\n",
    "MODEL_SAVE_PATH = os.path.join(GCS_BASE_PATH, 'models/two-tower-model')\n",
    "print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
    "tf.saved_model.save(serving_model, MODEL_SAVE_PATH)\n",
    "print(\"‚úÖ Model saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60460986",
   "metadata": {
    "cellView": "form",
    "id": "deploy_cell"
   },
   "outputs": [],
   "source": [
    "# @title üöÄ Step 8: Deploy to Cloud Run (Serverless)\n",
    "# @markdown We will create a Flask app, package it with Docker, and deploy it to Cloud Run.\n",
    "\n",
    "import os\n",
    "\n",
    "# 1. Create Deployment Directory\n",
    "os.makedirs(\"deploy_app\", exist_ok=True)\n",
    "\n",
    "# 2. Write Flask App\n",
    "app_code = f\"\"\"\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "from flask import Flask, request, jsonify\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "LOCAL_MODEL_PATH = \"/app/model\"\n",
    "MODEL_GCS_PATH = \"{MODEL_SAVE_PATH}\"\n",
    "\n",
    "# Download Model at startup\n",
    "print(\"Downloading model from GCS...\")\n",
    "os.system(f\"gsutil -m cp -r {{MODEL_GCS_PATH}}/* {{LOCAL_MODEL_PATH}}\")\n",
    "\n",
    "print(\"Loading model...\")\n",
    "model = tf.saved_model.load(LOCAL_MODEL_PATH)\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    try:\n",
    "        data = request.json\n",
    "        inputs = {{\n",
    "            \"customer_id\": tf.constant([data['customer_id']]),\n",
    "            \"age_bin\": tf.constant([data['age_bin']]),\n",
    "            \"month_of_year\": tf.constant([data['month_of_year']]),\n",
    "            \"week_of_month\": tf.constant([data['week_of_month']])\n",
    "        }}\n",
    "        scores, ids = model(inputs)\n",
    "        recommendations = [id.decode('utf-8') for id in ids.numpy()[0]]\n",
    "        return jsonify({{\"recommendations\": recommendations}})\n",
    "    except Exception as e:\n",
    "        return jsonify({{\"error\": str(e)}}), 500\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host='0.0.0.0', port=8080)\n",
    "\"\"\"\n",
    "\n",
    "with open(\"deploy_app/main.py\", \"w\") as f:\n",
    "    f.write(app_code)\n",
    "\n",
    "# 3. Write Dockerfile\n",
    "dockerfile_code = \"\"\"\n",
    "FROM python:3.9-slim\n",
    "RUN apt-get update && apt-get install -y curl gnupg\n",
    "RUN echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] http://packages.cloud.google.com/apt cloud-sdk main\" | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list && \\\n",
    "    curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key --keyring /usr/share/keyrings/cloud.google.gpg add - && \\\n",
    "    apt-get update -y && apt-get install google-cloud-sdk -y\n",
    "WORKDIR /app\n",
    "RUN mkdir -p /app/model\n",
    "RUN pip install flask gunicorn tensorflow tensorflow-recommenders scann\n",
    "COPY main.py .\n",
    "CMD exec gunicorn --bind :8080 --workers 1 --threads 8 --timeout 0 main:app\n",
    "\"\"\"\n",
    "\n",
    "with open(\"deploy_app/Dockerfile\", \"w\") as f:\n",
    "    f.write(dockerfile_code)\n",
    "\n",
    "print(\"‚úÖ Deployment files generated.\")\n",
    "\n",
    "# 4. Build and Deploy\n",
    "IMAGE_NAME = f\"gcr.io/{PROJECT_ID}/hm-recommender-app\"\n",
    "SERVICE_NAME = \"hm-recommender-service\"\n",
    "\n",
    "print(f\"üî® Building Container: {IMAGE_NAME}\")\n",
    "!gcloud builds submit --tag $IMAGE_NAME deploy_app\n",
    "\n",
    "print(f\"üöÄ Deploying to Cloud Run: {SERVICE_NAME}\")\n",
    "!gcloud run deploy $SERVICE_NAME \\\n",
    "  --image $IMAGE_NAME \\\n",
    "  --platform managed \\\n",
    "  --region $REGION \\\n",
    "  --allow-unauthenticated \\\n",
    "  --memory 2Gi\n",
    "\n",
    "print(\"‚úÖ Deployment Complete! Click the URL above to test your API.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
