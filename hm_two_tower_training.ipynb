{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da123af4",
   "metadata": {
    "cellView": "form",
    "id": "config_cell"
   },
   "outputs": [],
   "source": [
    "# @title ‚öôÔ∏è Workshop Configuration & Setup\n",
    "# @markdown Please enter your project details and training parameters below.\n",
    "# @markdown ---\n",
    "\n",
    "import os\n",
    "\n",
    "# @markdown ### ‚òÅÔ∏è Cloud Project Settings\n",
    "PROJECT_ID = \"your-project-id-here\" # @param {type:\"string\"}\n",
    "BUCKET_NAME = \"hm-recommendation-workshop\" # @param {type:\"string\"}\n",
    "REGION = \"us-central1\" # @param {type:\"string\"}\n",
    "\n",
    "# @markdown ### üöÄ Model Hyperparameters\n",
    "EMBEDDING_DIM = 64 # @param {type:\"integer\"}\n",
    "LEARNING_RATE = 0.1 # @param {type:\"number\"}\n",
    "EPOCHS = 5 # @param {type:\"slider\", min:1, max:10, step:1}\n",
    "\n",
    "# @markdown ### üì¶ Data Paths (Relative to Bucket)\n",
    "# @markdown Do not change these unless your bucket structure is different.\n",
    "ARTICLES_FILE = \"articles.csv\" # @param {type:\"string\"}\n",
    "CUSTOMERS_FILE = \"customers.csv\" # @param {type:\"string\"}\n",
    "TRANSACTIONS_FILE = \"transactions.csv\" # @param {type:\"string\"}\n",
    "\n",
    "# Setup Environment Variables\n",
    "os.environ[\"GCLOUD_PROJECT\"] = PROJECT_ID\n",
    "# Important: Use legacy Keras behavior for TF 2.x compatibility with ScaNN\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\" \n",
    "\n",
    "GCS_BASE_PATH = f\"gs://{BUCKET_NAME}\"\n",
    "\n",
    "print(f\"‚úÖ Configuration set for Project: {PROJECT_ID}\")\n",
    "print(f\"üìÇ Data Source: {GCS_BASE_PATH}\")\n",
    "print(f\"weights will be saved to: {GCS_BASE_PATH}/models/two-tower-model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe065d4d",
   "metadata": {
    "cellView": "form",
    "id": "install_cell"
   },
   "outputs": [],
   "source": [
    "# @title üì• Step 1: Install Libraries\n",
    "# @markdown Installing TensorFlow Recommenders, ScaNN, and Datasets.\n",
    "# @markdown This may take 1-2 minutes.\n",
    "\n",
    "import sys\n",
    "\n",
    "# Install TF-compatible version of ScaNN and other dependencies\n",
    "!pip install -q tensorflow-recommenders --no-deps\n",
    "!pip install -q --upgrade tensorflow-datasets\n",
    "!pip install -q \"scann[tf]\" tensorflow-recommenders tensorflow-datasets\n",
    "\n",
    "print(\"‚úÖ Installation Complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb7224f",
   "metadata": {
    "cellView": "form",
    "id": "data_load_cell"
   },
   "outputs": [],
   "source": [
    "# @title üíæ Step 2: Load Data from Cloud Storage\n",
    "# @markdown Reading CSV files directly from your GCS Bucket using the paths defined in configuration.\n",
    "\n",
    "import os\n",
    "import pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "from typing import Dict, Text, List\n",
    "from tensorflow.keras.layers import StringLookup, Embedding, Dense\n",
    "\n",
    "# GPU Check\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"üöÄ GPU Active: {gpus[0].name}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Running on CPU (Training might be slower)\")\n",
    "\n",
    "# Paths\n",
    "ARTICLES_PATH = os.path.join(GCS_BASE_PATH, ARTICLES_FILE)\n",
    "CUSTOMERS_PATH = os.path.join(GCS_BASE_PATH, CUSTOMERS_FILE)\n",
    "TRANSACTIONS_PATH = os.path.join(GCS_BASE_PATH, TRANSACTIONS_FILE)\n",
    "\n",
    "# --- 1. Load Articles ---\n",
    "print(f\"Loading Articles from: {ARTICLES_PATH}\")\n",
    "ARTICLE_FEATURES = ['article_id', 'product_type_name', 'product_group_name', 'colour_group_name', 'department_name']\n",
    "articles_df = pd.read_csv(ARTICLES_PATH, usecols=ARTICLE_FEATURES, dtype={'article_id': str})\n",
    "for col in ARTICLE_FEATURES:\n",
    "    if col != 'article_id':\n",
    "        articles_df[col] = articles_df[col].fillna('Unknown')\n",
    "\n",
    "# --- 2. Load Customers ---\n",
    "print(f\"Loading Customers from: {CUSTOMERS_PATH}\")\n",
    "customers_df = pd.read_csv(CUSTOMERS_PATH, usecols=['customer_id', 'age'])\n",
    "age_bins = [0, 19, 25, 29, 35, 39, 45, 49, 59, 69, 100]\n",
    "age_labels = ['0-19', '20-25', '26-29', '30-35', '36-39', '40-45', '46-49', '50-59', '60-69', '70+']\n",
    "customers_df['age_bin'] = pd.cut(customers_df['age'], bins=age_bins, labels=age_labels, right=False)\n",
    "customers_df['age_bin'] = customers_df['age_bin'].cat.add_categories('Unknown').fillna('Unknown').astype(str)\n",
    "\n",
    "# --- 3. Load Transactions ---\n",
    "print(f\"Loading Transactions from: {TRANSACTIONS_PATH}\")\n",
    "transactions_df = pd.read_csv(TRANSACTIONS_PATH, parse_dates=['t_dat'], dtype={'article_id': str})\n",
    "\n",
    "# Filter last year data\n",
    "val_start_date = pd.to_datetime('2020-09-09')\n",
    "train_start_date = val_start_date - pd.DateOffset(years=1)\n",
    "train_df = transactions_df[\n",
    "    (transactions_df['t_dat'] < val_start_date) & \n",
    "    (transactions_df['t_dat'] >= train_start_date)\n",
    "].copy()\n",
    "\n",
    "# Feature Engineering\n",
    "train_df['month_of_year'] = train_df['t_dat'].dt.month.astype(str)\n",
    "train_df['week_of_month'] = ((train_df['t_dat'].dt.day - 1) // 7 + 1).astype(str)\n",
    "interactions_df = train_df[['customer_id', 'article_id', 'month_of_year', 'week_of_month']]\n",
    "\n",
    "print(f\"‚úÖ Training dataset ready: {len(interactions_df)} rows.\")\n",
    "\n",
    "# Clean up RAM\n",
    "del transactions_df, train_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81de8a3a",
   "metadata": {
    "cellView": "form",
    "id": "prep_cell"
   },
   "outputs": [],
   "source": [
    "# @title üîß Step 3: Preprocessing & Lookup Tables\n",
    "# @markdown Creating string lookup tables for features (Age, Product Group, Department, etc.)\n",
    "\n",
    "customer_ids = customers_df['customer_id'].unique()\n",
    "article_ids = articles_df['article_id'].unique()\n",
    "age_groups = customers_df['age_bin'].unique()\n",
    "months = [str(i) for i in range(1, 13)]\n",
    "weeks = [str(i) for i in range(1, 6)]\n",
    "\n",
    "print(\"Creating Lookup Tables...\")\n",
    "cust_age_table = tf.lookup.StaticHashTable(\n",
    "    tf.lookup.KeyValueTensorInitializer(customers_df['customer_id'], customers_df['age_bin']),\n",
    "    default_value='Unknown'\n",
    ")\n",
    "\n",
    "article_tables = {}\n",
    "feature_cols = ['product_type_name', 'product_group_name', 'colour_group_name', 'department_name']\n",
    "for col in feature_cols:\n",
    "    article_tables[col] = tf.lookup.StaticHashTable(\n",
    "        tf.lookup.KeyValueTensorInitializer(articles_df['article_id'], articles_df[col]),\n",
    "        default_value='Unknown'\n",
    "    )\n",
    "\n",
    "# Create Datasets\n",
    "articles_ds = tf.data.Dataset.from_tensor_slices(dict(articles_df))\n",
    "interactions_ds = tf.data.Dataset.from_tensor_slices(dict(interactions_df))\n",
    "\n",
    "def add_features(features):\n",
    "    features['age_bin'] = cust_age_table.lookup(features['customer_id'])\n",
    "    for col in feature_cols:\n",
    "        features[col] = article_tables[col].lookup(features['article_id'])\n",
    "    return features\n",
    "\n",
    "interactions_ds = interactions_ds.map(add_features, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "print(\"‚úÖ Lookup tables and pipelines created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b966dbc3",
   "metadata": {
    "cellView": "form",
    "id": "model_def_cell"
   },
   "outputs": [],
   "source": [
    "# @title üß† Step 4: Define Two-Tower Model\n",
    "# @markdown Defining User Tower and Item Tower architectures using TensorFlow Recommenders.\n",
    "\n",
    "class UserModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.customer_id_lookup = StringLookup(vocabulary=customer_ids, mask_token=None)\n",
    "        self.customer_id_emb = Embedding(len(customer_ids) + 1, EMBEDDING_DIM)\n",
    "        \n",
    "        self.age_bin_lookup = StringLookup(vocabulary=age_groups, mask_token=None)\n",
    "        self.age_bin_emb = Embedding(len(age_groups) + 1, EMBEDDING_DIM // 4)\n",
    "        \n",
    "        self.month_lookup = StringLookup(vocabulary=months, mask_token=None)\n",
    "        self.month_emb = Embedding(len(months) + 1, EMBEDDING_DIM // 4)\n",
    "        \n",
    "        self.week_lookup = StringLookup(vocabulary=weeks, mask_token=None)\n",
    "        self.week_emb = Embedding(len(weeks) + 1, EMBEDDING_DIM // 4)\n",
    "        self.projection = Dense(EMBEDDING_DIM)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = tf.concat([\n",
    "            self.customer_id_emb(self.customer_id_lookup(inputs['customer_id'])),\n",
    "            self.age_bin_emb(self.age_bin_lookup(inputs['age_bin'])),\n",
    "            self.month_emb(self.month_lookup(inputs['month_of_year'])),\n",
    "            self.week_emb(self.week_lookup(inputs['week_of_month'])),\n",
    "        ], axis=1)\n",
    "        return self.projection(x)\n",
    "\n",
    "class ItemModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.article_id_lookup = StringLookup(vocabulary=article_ids, mask_token=None)\n",
    "        self.article_id_emb = Embedding(len(article_ids) + 1, EMBEDDING_DIM)\n",
    "        \n",
    "        self.lookups = {}\n",
    "        self.embeddings = {}\n",
    "        for col in feature_cols:\n",
    "            vocab = articles_df[col].unique()\n",
    "            self.lookups[col] = StringLookup(vocabulary=vocab, mask_token=None)\n",
    "            self.embeddings[col] = Embedding(len(vocab) + 1, EMBEDDING_DIM // 4)\n",
    "        self.projection = Dense(EMBEDDING_DIM)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        embs = [self.article_id_emb(self.article_id_lookup(inputs['article_id']))]\n",
    "        for col in feature_cols:\n",
    "            embs.append(self.embeddings[col](self.lookups[col](inputs[col])))\n",
    "        x = tf.concat(embs, axis=1)\n",
    "        return self.projection(x)\n",
    "\n",
    "class HMRModel(tfrs.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.user_model = UserModel()\n",
    "        self.item_model = ItemModel()\n",
    "        self.task = tfrs.tasks.Retrieval()\n",
    "\n",
    "    def compute_loss(self, features, training=False):\n",
    "        user_embeddings = self.user_model(features)\n",
    "        item_embeddings = self.item_model(features)\n",
    "        return self.task(user_embeddings, item_embeddings)\n",
    "\n",
    "print(\"‚úÖ Model architecture defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b48a33",
   "metadata": {
    "cellView": "form",
    "id": "train_cell"
   },
   "outputs": [],
   "source": [
    "# @title üèãÔ∏è Step 5: Train the Model\n",
    "# @markdown Training the Two-Tower model with Adagrad optimizer.\n",
    "# @markdown *Note: This uses the Epochs and Learning Rate defined in Step 1.*\n",
    "\n",
    "# Cache data in RAM\n",
    "cached_train = interactions_ds.shuffle(100_000).batch(16384).cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "model = HMRModel()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(LEARNING_RATE))\n",
    "\n",
    "print(f\"Starting training for {EPOCHS} epochs...\")\n",
    "history = model.fit(cached_train, epochs=EPOCHS)\n",
    "print(\"‚úÖ Training finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35718d42",
   "metadata": {
    "cellView": "form",
    "id": "scann_cell"
   },
   "outputs": [],
   "source": [
    "# @title üîç Step 6: Build ScaNN Index\n",
    "# @markdown Building Approximate Nearest Neighbor index for fast retrieval.\n",
    "\n",
    "print(\"Building ScaNN index...\")\n",
    "scann_index = tfrs.layers.factorized_top_k.ScaNN(\n",
    "    model.user_model,\n",
    "    num_reordering_candidates=500,\n",
    "    num_leaves=1000,\n",
    "    num_leaves_to_search=30,\n",
    "    k=50\n",
    ")\n",
    "\n",
    "# Index all items\n",
    "candidate_dataset = articles_ds.batch(2048).map(lambda x: (x[\"article_id\"], model.item_model(x)))\n",
    "scann_index.index_from_dataset(candidate_dataset)\n",
    "\n",
    "# Build check\n",
    "sample_query = {\n",
    "    \"customer_id\": tf.constant([customer_ids[0]]),\n",
    "    \"age_bin\": tf.constant([age_groups[0]]),\n",
    "    \"month_of_year\": tf.constant([\"9\"]),\n",
    "    \"week_of_month\": tf.constant([\"2\"])\n",
    "}\n",
    "_ = scann_index(sample_query)\n",
    "print(\"‚úÖ ScaNN index built successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9dc52d",
   "metadata": {
    "cellView": "form",
    "id": "save_cell"
   },
   "outputs": [],
   "source": [
    "# @title üíæ Step 7: Save Model to GCS\n",
    "# @markdown Saving the serving-ready model to Google Cloud Storage.\n",
    "\n",
    "class ServingModel(tf.keras.Model):\n",
    "    def __init__(self, index_layer):\n",
    "        super().__init__()\n",
    "        self.index_layer = index_layer\n",
    "\n",
    "    @tf.function(input_signature=[{\n",
    "        \"customer_id\": tf.TensorSpec(shape=(None,), dtype=tf.string),\n",
    "        \"age_bin\": tf.TensorSpec(shape=(None,), dtype=tf.string),\n",
    "        \"month_of_year\": tf.TensorSpec(shape=(None,), dtype=tf.string),\n",
    "        \"week_of_month\": tf.TensorSpec(shape=(None,), dtype=tf.string)\n",
    "    }])\n",
    "    def call(self, features):\n",
    "        return self.index_layer(features)\n",
    "\n",
    "serving_model = ServingModel(scann_index)\n",
    "_ = serving_model(sample_query) # Final build check\n",
    "\n",
    "MODEL_SAVE_PATH = os.path.join(GCS_BASE_PATH, 'models/two-tower-model')\n",
    "print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
    "tf.saved_model.save(serving_model, MODEL_SAVE_PATH)\n",
    "print(\"‚úÖ Model saved successfully.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
