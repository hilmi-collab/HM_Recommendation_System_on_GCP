{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f698c718",
   "metadata": {
    "cellView": "form",
    "id": "config_cell"
   },
   "outputs": [],
   "source": [
    "# @title ‚öôÔ∏è Ranking Model Configuration\n",
    "# @markdown Enter your project details below.\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import lightgbm as lgb\n",
    "from datetime import timedelta\n",
    "\n",
    "# @markdown ### ‚òÅÔ∏è Cloud Project Settings\n",
    "PROJECT_ID = \"your-project-id-here\" # @param {type:\"string\"}\n",
    "BUCKET_NAME = \"hm-recommendation-workshop\" # @param {type:\"string\"}\n",
    "REGION = \"us-central1\" # @param {type:\"string\"}\n",
    "\n",
    "# @markdown ### üß™ Experiment Settings\n",
    "TOP_K_RETRIEVAL = 60 # @param {type:\"integer\"}\n",
    "NUM_TRAIN_WEEKS = 6 # @param {type:\"slider\", min:1, max:6}\n",
    "LEARNING_RATE = 0.005 # @param {type:\"number\"}\n",
    "NUM_LEAVES = 255 # @param {type:\"integer\"}\n",
    "NUM_ROUNDS = 5000 # @param {type:\"integer\"}\n",
    "\n",
    "# Environment & Paths\n",
    "os.environ[\"GCLOUD_PROJECT\"] = PROJECT_ID\n",
    "BASE_PATH = f'gs://{BUCKET_NAME}'\n",
    "# Output path for artifacts\n",
    "ARTIFACTS_PATH = os.path.join(BASE_PATH, 'models/ranking_model') \n",
    "\n",
    "# Raw Data Paths\n",
    "ARTICLES_PATH = os.path.join(BASE_PATH, 'articles.csv')\n",
    "CUSTOMERS_PATH = os.path.join(BASE_PATH, 'customers.csv')\n",
    "TRANSACTIONS_PATH = os.path.join(BASE_PATH, 'transactions.csv')\n",
    "RETRIEVAL_MODEL_PATH = os.path.join(BASE_PATH, 'models/two-tower-model')\n",
    "\n",
    "print(f\"‚úÖ Configuration set for Project: {PROJECT_ID}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdd27aa",
   "metadata": {
    "cellView": "form",
    "id": "install_cell"
   },
   "outputs": [],
   "source": [
    "# @title üì• Step 1: Install Dependencies\n",
    "# @markdown Installing necessary libraries.\n",
    "# @markdown Added `gcsfs` to read directly from GCS via Pandas.\n",
    "\n",
    "!pip uninstall -y protobuf > /dev/null\n",
    "!pip install protobuf==3.20.3 > /dev/null\n",
    "!pip install -q tensorflow-recommenders lightgbm pandas numpy gcsfs\n",
    "\n",
    "print(\"‚úÖ Installation Complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4326a1d5",
   "metadata": {
    "cellView": "form",
    "id": "metrics_cell"
   },
   "outputs": [],
   "source": [
    "# @title üõ†Ô∏è Step 2: Helper Functions (MAP@12)\n",
    "def apk(actual, predicted, k=10):\n",
    "    if len(predicted) > k: predicted = predicted[:k]\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "    for i, p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i + 1.0)\n",
    "    if not actual: return 0.0\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual, predicted, k=10):\n",
    "    return np.mean([apk(a, p, k) for a, p in zip(actual, predicted)])\n",
    "\n",
    "print(\"‚úÖ Metric functions defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199e69da",
   "metadata": {
    "cellView": "form",
    "id": "load_static_cell"
   },
   "outputs": [],
   "source": [
    "# @title üíæ Step 3: Load Static Data (Training)\n",
    "# @markdown Loading Articles and Customers for training process.\n",
    "\n",
    "def load_static_data():\n",
    "    print(\">>> Loading Static Data (Articles & Customers)...\")\n",
    "    \n",
    "    # 1. Articles\n",
    "    cols = ['article_id', 'product_code', 'product_type_name', 'product_group_name',\n",
    "            'graphical_appearance_no', 'colour_group_code', 'section_no', 'garment_group_no']\n",
    "    articles = pd.read_csv(ARTICLES_PATH, dtype={'article_id': str}, usecols=cols)\n",
    "    \n",
    "    for c in cols:\n",
    "        if c != 'article_id': articles[c] = pd.factorize(articles[c].astype(str), sort=True)[0]\n",
    "\n",
    "    articles['article_id_int'], _ = pd.factorize(articles['article_id'], sort=True)\n",
    "    article_map = dict(zip(articles['article_id'], articles['article_id_int']))\n",
    "    \n",
    "    articles['article_id'] = articles['article_id_int']\n",
    "    del articles['article_id_int']\n",
    "\n",
    "    # 2. Customers\n",
    "    cust_cols = ['customer_id', 'FN', 'Active', 'age', 'club_member_status']\n",
    "    customers = pd.read_csv(CUSTOMERS_PATH, usecols=cust_cols, dtype={'customer_id': str})\n",
    "    \n",
    "    customers['FN'] = customers['FN'].fillna(0)\n",
    "    customers['Active'] = customers['Active'].fillna(0)\n",
    "    customers['age'] = customers['age'].fillna(customers['age'].mean())\n",
    "    customers['club_member_status'] = pd.factorize(customers['club_member_status'].fillna('Unknown'), sort=True)[0]\n",
    "\n",
    "    customers['customer_id_int'], _ = pd.factorize(customers['customer_id'], sort=True)\n",
    "    customer_map = dict(zip(customers['customer_id'], customers['customer_id_int']))\n",
    "    customers['customer_id'] = customers['customer_id_int']\n",
    "    del customers['customer_id_int']\n",
    "\n",
    "    return articles, customers, article_map, customer_map\n",
    "\n",
    "articles_df, customers_df, article_map, customer_map = load_static_data()\n",
    "print(\"‚úÖ Static data loaded and mapped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55800e30",
   "metadata": {
    "cellView": "form",
    "id": "data_engine_cell"
   },
   "outputs": [],
   "source": [
    "# @title üè≠ Step 4: Data Generation Engine\n",
    "# @markdown Helper function to generate candidate datasets using Two-Tower retrieval.\n",
    "\n",
    "def generate_weekly_data(target_start_date, df_trans, tf_model, is_training=True):\n",
    "    history_cutoff = target_start_date\n",
    "    target_end_date = target_start_date + timedelta(days=7)\n",
    "    \n",
    "    df_history = df_trans[df_trans['t_dat'] < history_cutoff]\n",
    "\n",
    "    if is_training:\n",
    "        df_target = df_trans[(df_trans['t_dat'] >= target_start_date) & (df_trans['t_dat'] < target_end_date)]\n",
    "        target_users = df_target['customer_id'].unique()\n",
    "    else:\n",
    "        df_target = df_trans[df_trans['t_dat'] >= target_start_date]\n",
    "        target_users = df_target['customer_id'].unique()\n",
    "\n",
    "    if len(target_users) == 0: return None\n",
    "\n",
    "    # Popularity & Stats\n",
    "    last_week_start = history_cutoff - timedelta(days=7)\n",
    "    df_last_week = df_history[df_history['t_dat'] > last_week_start]\n",
    "    item_trend_score = df_last_week.groupby('article_id').size().reset_index(name='trend_score')\n",
    "\n",
    "    hist_age = df_history[['article_id', 'customer_id']].merge(customers_df[['customer_id', 'age']], on='customer_id')\n",
    "    item_avg_age = hist_age.groupby('article_id')['age'].mean().reset_index(name='item_avg_age')\n",
    "\n",
    "    # Candidates\n",
    "    top_items = item_trend_score.sort_values('trend_score', ascending=False).head(12)['article_id'].tolist()\n",
    "    \n",
    "    repurchase_start = history_cutoff - timedelta(days=28)\n",
    "    df_rep = df_history[(df_history['t_dat'] > repurchase_start) & (df_history['customer_id'].isin(target_users))]\n",
    "    user_history = df_rep.groupby('customer_id')['article_id'].apply(lambda x: list(set(x))).to_dict()\n",
    "\n",
    "    # Retrieval\n",
    "    inv_cust_map = {v: k for k, v in customer_map.items()}\n",
    "    tf_cands_dict = {}\n",
    "    tf_scores_dict = {}\n",
    "    \n",
    "    BATCH = 1000\n",
    "    tgt_list = list(target_users)\n",
    "\n",
    "    for i in range(0, len(tgt_list), BATCH):\n",
    "        batch_uids = tgt_list[i:i+BATCH]\n",
    "        batch_strs = [inv_cust_map[u] for u in batch_uids]\n",
    "        \n",
    "        inp = {\n",
    "            \"customer_id\": tf.constant(batch_strs),\n",
    "            \"age_bin\": tf.constant([\"25\"]*len(batch_strs)),\n",
    "            \"month_of_year\": tf.constant([\"9\"]*len(batch_strs)),\n",
    "            \"week_of_month\": tf.constant([\"2\"]*len(batch_strs))\n",
    "        }\n",
    "        res = tf_model(inp)\n",
    "        cands = res['candidates'].numpy().astype(str)\n",
    "        scores = res['scores'].numpy()\n",
    "\n",
    "        for idx, u in enumerate(batch_uids):\n",
    "            c_list = []\n",
    "            s_map = {}\n",
    "            for j in range(min(TOP_K_RETRIEVAL, len(cands[idx]))):\n",
    "                art_str = cands[idx][j]\n",
    "                if art_str in article_map:\n",
    "                    art_int = article_map[art_str]\n",
    "                    c_list.append(art_int)\n",
    "                    s_map[art_int] = float(scores[idx][j])\n",
    "            tf_cands_dict[u] = c_list\n",
    "            tf_scores_dict[u] = s_map\n",
    "\n",
    "    # Merge\n",
    "    data = []\n",
    "    for u in target_users:\n",
    "        candidates = set()\n",
    "        candidates.update(top_items)\n",
    "        if u in user_history: candidates.update(user_history[u])\n",
    "        if u in tf_cands_dict: candidates.update(tf_cands_dict[u])\n",
    "\n",
    "        for aid in candidates:\n",
    "            t_score = tf_scores_dict.get(u, {}).get(aid, 0.0)\n",
    "            data.append([u, aid, t_score])\n",
    "\n",
    "    df_week = pd.DataFrame(data, columns=['customer_id', 'article_id', 'tf_score'])\n",
    "\n",
    "    if is_training:\n",
    "        df_target['purchased'] = 1\n",
    "        truth = df_target[['customer_id', 'article_id', 'purchased']].drop_duplicates()\n",
    "        df_week = df_week.merge(truth, on=['customer_id', 'article_id'], how='left')\n",
    "        df_week['label'] = df_week['purchased'].fillna(0).astype('int8')\n",
    "        del df_week['purchased']\n",
    "\n",
    "        pos = df_week[df_week['label'] == 1]\n",
    "        neg = df_week[df_week['label'] == 0]\n",
    "        if len(neg) > 1_500_000:\n",
    "            neg = neg.sample(n=1_500_000, random_state=42)\n",
    "        df_week = pd.concat([pos, neg])\n",
    "\n",
    "    # Features\n",
    "    df_week = df_week.merge(item_trend_score, on='article_id', how='left').fillna({'trend_score': 0})\n",
    "    df_week = df_week.merge(customers_df[['customer_id', 'age']], on='customer_id', how='left')\n",
    "    df_week = df_week.merge(item_avg_age, on='article_id', how='left')\n",
    "    df_week['item_avg_age'] = df_week['item_avg_age'].fillna(30)\n",
    "    df_week['age_diff'] = np.abs(df_week['age'] - df_week['item_avg_age'])\n",
    "    df_week = df_week.merge(articles_df, on='article_id', how='left')\n",
    "    cust_cols_static = [c for c in customers_df.columns if c not in ['age', 'customer_id']]\n",
    "    df_week = df_week.merge(customers_df[['customer_id'] + cust_cols_static], on='customer_id', how='left')\n",
    "\n",
    "    return df_week\n",
    "\n",
    "print(\"‚úÖ Data Generation Engine ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e9d954",
   "metadata": {
    "cellView": "form",
    "id": "train_eval_cell"
   },
   "outputs": [],
   "source": [
    "# @title üèãÔ∏è Step 5: Train LightGBM Model\n",
    "# @markdown Loads transactions, prepares data, and trains the LightGBM Ranker.\n",
    "\n",
    "print(\">>> Loading Transactions...\")\n",
    "df_trans = pd.read_csv(TRANSACTIONS_PATH, dtype={'article_id': str, 'customer_id': str}, parse_dates=['t_dat'])\n",
    "df_trans['article_id'] = df_trans['article_id'].map(article_map).fillna(-1).astype('int32')\n",
    "df_trans['customer_id'] = df_trans['customer_id'].map(customer_map).fillna(-1).astype('int32')\n",
    "df_trans = df_trans[(df_trans['article_id'] != -1) & (df_trans['customer_id'] != -1)]\n",
    "\n",
    "print(\">>> Loading Retrieval Model...\")\n",
    "if not os.path.exists(\"two-tower-model\"):\n",
    "    os.system(f\"gsutil -m cp -r {RETRIEVAL_MODEL_PATH} two-tower-model\")\n",
    "tf_model = tf.saved_model.load(\"two-tower-model\")\n",
    "\n",
    "print(f\"Generating training data for {NUM_TRAIN_WEEKS} weeks...\")\n",
    "VAL_WEEK_START = pd.to_datetime('2020-09-16')\n",
    "\n",
    "big_train_df = pd.DataFrame()\n",
    "for w in range(1, NUM_TRAIN_WEEKS + 1):\n",
    "    target_start = VAL_WEEK_START - timedelta(weeks=w)\n",
    "    print(f\"   Processing Week: {target_start.date()}\")\n",
    "    week_df = generate_weekly_data(target_start, df_trans, tf_model, is_training=True)\n",
    "    if week_df is not None:\n",
    "        big_train_df = pd.concat([big_train_df, week_df])\n",
    "        del week_df\n",
    "        gc.collect()\n",
    "\n",
    "print(\"Preparing LightGBM Dataset...\")\n",
    "big_train_df = big_train_df.sort_values(by=['customer_id'], kind='mergesort')\n",
    "drop_cols = ['customer_id', 'article_id', 'label']\n",
    "X = big_train_df.drop(columns=drop_cols)\n",
    "y = big_train_df['label']\n",
    "group = big_train_df.groupby('customer_id', sort=False).size().to_numpy()\n",
    "\n",
    "cat_cols = ['product_code', 'product_type_name', 'product_group_name',\n",
    "            'graphical_appearance_no', 'colour_group_code', 'section_no',\n",
    "            'garment_group_no', 'club_member_status']\n",
    "\n",
    "train_set = lgb.Dataset(X, y, group=group, categorical_feature=cat_cols, free_raw_data=False)\n",
    "\n",
    "params = {\n",
    "    'objective': 'lambdarank',\n",
    "    'metric': 'map',\n",
    "    'eval_at': [12],\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'num_leaves': NUM_LEAVES,\n",
    "    'max_depth': -1,\n",
    "    'feature_fraction': 0.7,\n",
    "    'bagging_fraction': 0.7,\n",
    "    'bagging_freq': 1,\n",
    "    'force_col_wise': True,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "print(\">>> Starting Training...\")\n",
    "model = lgb.train(params, train_set, num_boost_round=NUM_ROUNDS)\n",
    "model.save_model(\"model.model\")\n",
    "\n",
    "# Evaluate\n",
    "print(\">>> Evaluating...\")\n",
    "eval_df = generate_weekly_data(VAL_WEEK_START, df_trans, tf_model, is_training=False)\n",
    "model_features = model.feature_name()\n",
    "for c in model_features:\n",
    "    if c not in eval_df.columns: eval_df[c] = 0\n",
    "\n",
    "X_eval = eval_df[model_features]\n",
    "eval_df['score'] = model.predict(X_eval)\n",
    "\n",
    "top_recs = eval_df.sort_values(['customer_id', 'score'], ascending=[True, False]).groupby('customer_id').head(12)\n",
    "preds_map = top_recs.groupby('customer_id')['article_id'].apply(list).to_dict()\n",
    "\n",
    "val_target_df = df_trans[df_trans['t_dat'] >= VAL_WEEK_START]\n",
    "ground_truth = val_target_df.groupby('customer_id')['article_id'].apply(list).to_dict()\n",
    "val_users = list(ground_truth.keys())\n",
    "\n",
    "map_scores = [apk(ground_truth[uid], preds_map[uid], k=12) if uid in preds_map else 0.0 for uid in val_users]\n",
    "final_map = np.mean(map_scores)\n",
    "print(f\"üéâ FINAL MAP@12 SCORE: {final_map:.5f}\")\n",
    "\n",
    "# Upload Model\n",
    "print(f\"Uploading model to {ARTIFACTS_PATH}...\")\n",
    "os.system(f\"gsutil cp model.model {ARTIFACTS_PATH}/model.model\")\n",
    "\n",
    "print(\"‚úÖ Training and Evaluation complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c412cdf1",
   "metadata": {
    "cellView": "form",
    "id": "prep_serving_cell"
   },
   "outputs": [],
   "source": [
    "# @title üì¶ Step 6: Prepare & Upload Serving Artifacts\n",
    "# @markdown This step processes the raw data into optimized Parquet files for the Serving API.\n",
    "# @markdown It generates:\n",
    "# @markdown * `app_articles.parquet` (Product details + Images)\n",
    "# @markdown * `app_customers.parquet` (User details)\n",
    "# @markdown * `app_user_history.parquet` (User's recent purchases)\n",
    "# @markdown * `app_stats.parquet` (Trending items)\n",
    "\n",
    "def generate_image_url(article_id):\n",
    "    # Hopsworks logic\n",
    "    article_id_str = str(article_id)\n",
    "    if len(article_id_str) == 9:\n",
    "        article_id_str = \"0\" + article_id_str\n",
    "    folder = article_id_str[:2]\n",
    "    return f\"https://repo.hops.works/dev/jdowling/h-and-m/images/{folder}/{article_id_str}.jpg\"\n",
    "\n",
    "print(\"1. Processing Articles for Serving...\")\n",
    "df_articles = pd.read_csv(ARTICLES_PATH, dtype={'article_id': str})\n",
    "df_articles['image_url'] = df_articles['article_id'].apply(generate_image_url)\n",
    "\n",
    "# Add missing serving cols\n",
    "if 'trend_score' not in df_articles.columns: df_articles['trend_score'] = 0.5 \n",
    "if 'item_avg_age' not in df_articles.columns: df_articles['item_avg_age'] = 30.0\n",
    "\n",
    "# Optimize types\n",
    "for col in df_articles.select_dtypes(include=['object']).columns:\n",
    "    if col not in ['article_id', 'prod_name', 'image_url', 'detail_desc']:\n",
    "        df_articles[col] = df_articles[col].astype('category')\n",
    "        \n",
    "df_articles.to_parquet('app_articles.parquet', index=False)\n",
    "print(\"   -> app_articles.parquet created.\")\n",
    "\n",
    "print(\"2. Processing Customers for Serving...\")\n",
    "df_customers = pd.read_csv(CUSTOMERS_PATH, dtype={'customer_id': str})\n",
    "df_customers['age'] = df_customers['age'].fillna(df_customers['age'].mean())\n",
    "df_customers['club_member_status'] = df_customers['club_member_status'].fillna('Unknown')\n",
    "df_customers.to_parquet('app_customers.parquet', index=False)\n",
    "print(\"   -> app_customers.parquet created.\")\n",
    "\n",
    "print(\"3. Processing User History (Last 28 Days)...\")\n",
    "# Re-read transactions with original string IDs for serving mapping\n",
    "df_tr = pd.read_csv(TRANSACTIONS_PATH, usecols=['t_dat', 'customer_id', 'article_id'],\n",
    "                 dtype={'article_id': str, 'customer_id': str},\n",
    "                 parse_dates=['t_dat'])\n",
    "VAL_START = pd.to_datetime('2020-09-16')\n",
    "hist_start = VAL_START - timedelta(days=28)\n",
    "df_hist = df_tr[(df_tr['t_dat'] >= hist_start) & (df_tr['t_dat'] < VAL_START)]\n",
    "user_history = df_hist.groupby('customer_id')['article_id'].apply(lambda x: list(set(x))).reset_index()\n",
    "user_history.columns = ['customer_id', 'article_ids']\n",
    "user_history.to_parquet('app_user_history.parquet', index=False)\n",
    "print(\"   -> app_user_history.parquet created.\")\n",
    "\n",
    "print(\"4. Processing Validation Truth (For Analysis)...\")\n",
    "val_df = df_tr[df_tr['t_dat'] >= VAL_START]\n",
    "val_df[['customer_id', 'article_id']].to_parquet('val_truth.parquet', index=False)\n",
    "print(\"   -> val_truth.parquet created.\")\n",
    "\n",
    "print(\"5. Generating Stats (Trending Items)...\")\n",
    "# Calculate trending items from the last week of training data\n",
    "last_week_start = VAL_START - timedelta(days=7)\n",
    "df_trend = df_tr[(df_tr['t_dat'] >= last_week_start) & (df_tr['t_dat'] < VAL_START)]\n",
    "item_stats = df_trend.groupby('article_id').size().reset_index(name='trend_score')\n",
    "# Simple age proxy\n",
    "item_stats['item_avg_age'] = 30.0 \n",
    "item_stats.to_parquet('app_stats.parquet', index=False)\n",
    "print(\"   -> app_stats.parquet created.\")\n",
    "\n",
    "# --- UPLOAD ---\n",
    "print(f\"Uploading all serving artifacts to {ARTIFACTS_PATH}...\")\n",
    "os.system(f\"gsutil cp app_*.parquet {ARTIFACTS_PATH}/\")\n",
    "os.system(f\"gsutil cp val_truth.parquet {ARTIFACTS_PATH}/\")\n",
    "\n",
    "print(\"‚úÖ Serving artifacts are ready in GCS.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6751c4e",
   "metadata": {
    "cellView": "form",
    "id": "deploy_cell"
   },
   "outputs": [],
   "source": [
    "# @title üöÄ Step 7: Deploy Hybrid System to Cloud Run\n",
    "# @markdown We will deploy the full Hybrid System (Two-Tower Retrieval + LightGBM Ranking).\n",
    "# @markdown The app will automatically download the Parquet files we just created.\n",
    "\n",
    "import os\n",
    "\n",
    "# 1. Create Deployment Directory\n",
    "os.makedirs(\"deploy_app\", exist_ok=True)\n",
    "\n",
    "# 2. Write FastAPI App (main.py)\n",
    "app_code = f\"\"\"\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import tensorflow as tf\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import contextlib\n",
    "import traceback\n",
    "import gc\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "BUCKET_NAME = '{BUCKET_NAME}' \n",
    "GCS_BASE = f'gs://{{BUCKET_NAME}}'\n",
    "ARTIFACTS_PATH = f'{{GCS_BASE}}/models/ranking_model'\n",
    "TF_PATH = f'{{GCS_BASE}}/models/two-tower-model'\n",
    "\n",
    "# These match exactly what we created in Step 6\n",
    "PARQUET_FILES = [\n",
    "    'app_articles.parquet',\n",
    "    'app_customers.parquet',\n",
    "    'app_stats.parquet', \n",
    "    'app_user_history.parquet'\n",
    "]\n",
    "\n",
    "models = {{}}\n",
    "data = {{}}\n",
    "TOP_K_TREND = 12 \n",
    "\n",
    "@contextlib.asynccontextmanager\n",
    "async def lifespan(app: FastAPI):\n",
    "    print(\">>> [INIT] Starting up...\")\n",
    "    try:\n",
    "        # --- A. DOWNLOAD ARTIFACTS ---\n",
    "        print(\">>> [DOWNLOAD] Downloading artifacts...\")\n",
    "        \n",
    "        # 1. Download Parquet Files from Artifacts Path\n",
    "        for p_file in PARQUET_FILES:\n",
    "            src = f\"{{ARTIFACTS_PATH}}/{{p_file}}\"\n",
    "            os.system(f\"gsutil cp {{src}} .\")\n",
    "\n",
    "        # 2. Download LightGBM Model\n",
    "        if not os.path.exists(\"model.model\"):\n",
    "            os.system(f\"gsutil cp {{ARTIFACTS_PATH}}/model.model model.model\")\n",
    "\n",
    "        # 3. Download Two-Tower Model\n",
    "        if not os.path.exists(\"two-tower-model\"):\n",
    "            os.system(f\"gsutil -m cp -r {{TF_PATH}} two-tower-model\")\n",
    "\n",
    "        # --- B. LOAD DATA ---\n",
    "        print(\">>> [LOAD] Loading Parquet files...\")\n",
    "        if os.path.exists('app_articles.parquet'):\n",
    "            data['articles'] = pd.read_parquet('app_articles.parquet')\n",
    "            data['customers'] = pd.read_parquet('app_customers.parquet')\n",
    "            \n",
    "            # History Map\n",
    "            if os.path.exists('app_user_history.parquet'):\n",
    "                h_df = pd.read_parquet('app_user_history.parquet')\n",
    "                data['user_history_map'] = dict(zip(h_df['customer_id'], h_df['article_ids']))\n",
    "            else:\n",
    "                data['user_history_map'] = {{}}\n",
    "\n",
    "            # Stats & Trends\n",
    "            if os.path.exists('app_stats.parquet'):\n",
    "                stats = pd.read_parquet('app_stats.parquet')\n",
    "                data['stats'] = stats\n",
    "                data['top_trend_items'] = stats.sort_values('trend_score', ascending=False).head(TOP_K_TREND)['article_id'].tolist()\n",
    "            else:\n",
    "                data['stats'] = pd.DataFrame()\n",
    "                data['top_trend_items'] = []\n",
    "\n",
    "            # Mapping for LightGBM (String ID -> Int ID)\n",
    "            # We recreate mapping on the fly to ensure consistency\n",
    "            print(\">>> [MAP] Creating ID mappings...\")\n",
    "            data['articles']['article_id_idx'], _ = pd.factorize(data['articles']['article_id'], sort=True)\n",
    "            data['article_map'] = dict(zip(data['articles']['article_id'], data['articles']['article_id_idx']))\n",
    "            \n",
    "            # Map Stats\n",
    "            if not data['stats'].empty:\n",
    "                data['stats']['article_id_idx'] = data['stats']['article_id'].map(data['article_map'])\n",
    "                data['stats'] = data['stats'].dropna().rename(columns={{'trend_score': 'stat_trend', 'item_avg_age': 'stat_age'}})\n",
    "                data['stats']['article_id_idx'] = data['stats']['article_id_idx'].astype(int)\n",
    "\n",
    "            print(\">>> [DATA] Ready.\")\n",
    "        else:\n",
    "            print(\"!!! [CRITICAL] Parquet files missing.\")\n",
    "\n",
    "        # --- C. LOAD MODELS ---\n",
    "        print(\">>> [MODELS] Loading models...\")\n",
    "        if os.path.exists(\"model.model\"):\n",
    "            models['lgb'] = lgb.Booster(model_file=\"model.model\")\n",
    "        \n",
    "        if os.path.exists(\"two-tower-model\"):\n",
    "            models['tf'] = tf.saved_model.load(\"two-tower-model\")\n",
    "            \n",
    "        print(\">>> [READY] Service is ready.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"!!! [INIT ERROR] {{e}}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "    yield\n",
    "    models.clear()\n",
    "    data.clear()\n",
    "    gc.collect()\n",
    "\n",
    "app = FastAPI(lifespan=lifespan)\n",
    "\n",
    "class RecRequest(BaseModel):\n",
    "    user_id: str\n",
    "    history: list[str] = []\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "async def predict(req: RecRequest):\n",
    "    try:\n",
    "        user_id = req.user_id\n",
    "        candidates = set()\n",
    "        \n",
    "        # 1. CANDIDATES\n",
    "        # A. Trends\n",
    "        if 'top_trend_items' in data: candidates.update(data['top_trend_items'])\n",
    "        \n",
    "        # B. Two-Tower\n",
    "        try:\n",
    "            if 'tf' in models:\n",
    "                inp = {{\n",
    "                    \"customer_id\": tf.constant([user_id]),\n",
    "                    \"age_bin\": tf.constant([\"25\"]), \n",
    "                    \"month_of_year\": tf.constant([\"9\"]), \n",
    "                    \"week_of_month\": tf.constant([\"2\"])\n",
    "                }}\n",
    "                tf_res = models['tf'](inp)['candidates'].numpy()[0].astype(str)\n",
    "                candidates.update(tf_res)\n",
    "        except: pass\n",
    "\n",
    "        # C. User History\n",
    "        if 'user_history_map' in data and user_id in data['user_history_map']:\n",
    "            past_items = data['user_history_map'][user_id]\n",
    "            if isinstance(past_items, (np.ndarray, list)): candidates.update(past_items)\n",
    "            else: candidates.add(str(past_items))\n",
    "            \n",
    "        # D. Session History\n",
    "        if req.history: candidates.update(req.history)\n",
    "\n",
    "        if not candidates: return {{\"recommendations\": []}}\n",
    "\n",
    "        # 2. DATAFRAME CONSTRUCTION\n",
    "        valid_cands, valid_idxs = [], []\n",
    "        article_map = data.get('article_map', {{}})\n",
    "        \n",
    "        for c in candidates:\n",
    "            c_str = str(c).strip().zfill(10)\n",
    "            if c_str in article_map:\n",
    "                valid_cands.append(c_str)\n",
    "                valid_idxs.append(article_map[c_str])\n",
    "\n",
    "        if not valid_cands: return {{\"recommendations\": []}}\n",
    "\n",
    "        cand_df = pd.DataFrame({{'article_id': valid_cands, 'article_id_idx': valid_idxs}})\n",
    "        \n",
    "        # Merge Features\n",
    "        # Note: In serving we optimize by merging only necessary columns\n",
    "        # Assuming data['articles'] has static features\n",
    "        cand_df = cand_df.merge(data['articles'], on='article_id', how='left')\n",
    "        if 'stats' in data and not data['stats'].empty:\n",
    "            cand_df = cand_df.merge(data['stats'], on='article_id_idx', how='left')\n",
    "        \n",
    "        cand_df['trend_score'] = cand_df.get('stat_trend', 0.0).fillna(0)\n",
    "        cand_df['item_avg_age'] = cand_df.get('stat_age', 30.0).fillna(30.0)\n",
    "        \n",
    "        # User Features (Simple lookup)\n",
    "        cust_df = data['customers']\n",
    "        if user_id in cust_df['customer_id'].values:\n",
    "            u_row = cust_df[cust_df['customer_id'] == user_id].iloc[0]\n",
    "            cand_df['age'] = u_row['age']\n",
    "            cand_df['club_member_status'] = u_row['club_member_status']\n",
    "        else:\n",
    "            cand_df['age'] = 30.0\n",
    "            cand_df['club_member_status'] = 0\n",
    "\n",
    "        cand_df['age_diff'] = np.abs(cand_df['age'] - cand_df['item_avg_age'])\n",
    "\n",
    "        # 3. PREDICT\n",
    "        if 'lgb' in models:\n",
    "            feats = models['lgb'].feature_name()\n",
    "            for f in feats:\n",
    "                if f not in cand_df.columns: cand_df[f] = 0\n",
    "                if cand_df[f].dtype == 'object' or str(cand_df[f].dtype) == 'category':\n",
    "                    cand_df[f] = pd.factorize(cand_df[f])[0]\n",
    "            cand_df['score'] = models['lgb'].predict(cand_df[feats])\n",
    "        else:\n",
    "            cand_df['score'] = cand_df['trend_score']\n",
    "\n",
    "        # 4. RESPONSE\n",
    "        top_recs = cand_df.sort_values('score', ascending=False).head(12)\n",
    "        results = []\n",
    "        for _, row in top_recs.iterrows():\n",
    "            results.append({{\n",
    "                \"article_id\": row['article_id'],\n",
    "                \"prod_name\": str(row.get('prod_name', 'Unknown')),\n",
    "                \"image_url\": str(row.get('image_url', '')),\n",
    "                \"score\": float(row['score'])\n",
    "            }})\n",
    "            \n",
    "        return {{\"recommendations\": results}}\n",
    "\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        return {{ \"error\": str(e) }}, 500\n",
    "\"\"\"\n",
    "\n",
    "with open(\"deploy_app/main.py\", \"w\") as f:\n",
    "    f.write(app_code)\n",
    "\n",
    "# 3. Write Dockerfile\n",
    "dockerfile_code = \"\"\"\n",
    "FROM python:3.9-slim\n",
    "RUN apt-get update && apt-get install -y curl gnupg libgomp1\n",
    "RUN echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] http://packages.cloud.google.com/apt cloud-sdk main\" | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list && \\\n",
    "    curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key --keyring /usr/share/keyrings/cloud.google.gpg add - && \\\n",
    "    apt-get update -y && apt-get install google-cloud-sdk -y\n",
    "WORKDIR /app\n",
    "RUN pip install flask gunicorn tensorflow tensorflow-recommenders scann lightgbm pandas pyarrow fastapi uvicorn gcsfs\n",
    "COPY main.py .\n",
    "CMD exec uvicorn main:app --host 0.0.0.0 --port 8080\n",
    "\"\"\"\n",
    "\n",
    "with open(\"deploy_app/Dockerfile\", \"w\") as f:\n",
    "    f.write(dockerfile_code)\n",
    "\n",
    "print(\"‚úÖ Deployment files generated.\")\n",
    "\n",
    "# 4. Build and Deploy\n",
    "IMAGE_NAME = f\"gcr.io/{PROJECT_ID}/hm-recommender-app\"\n",
    "SERVICE_NAME = \"hm-recommender-service\"\n",
    "\n",
    "print(f\"üî® Building Container: {IMAGE_NAME}\")\n",
    "!gcloud builds submit --tag $IMAGE_NAME deploy_app\n",
    "\n",
    "print(f\"üöÄ Deploying to Cloud Run: {SERVICE_NAME}\")\n",
    "!gcloud run deploy $SERVICE_NAME \\\n",
    "  --image $IMAGE_NAME \\\n",
    "  --platform managed \\\n",
    "  --region $REGION \\\n",
    "  --allow-unauthenticated \\\n",
    "  --memory 4Gi\n",
    "\n",
    "print(\"‚úÖ Deployment Complete! Check the URL.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
