{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# @title âš™ï¸ Ranking Model Configuration\n",
        "import os\n",
        "from datetime import timedelta\n",
        "\n",
        "# @markdown ### â˜ï¸ Project Settings\n",
        "PROJECT_ID = \"YOUR_PROJECT_ID\" # @param {type:\"string\"}\n",
        "REGION = \"us-central1\" # @param {type:\"string\"}\n",
        "\n",
        "# 1. PUBLIC DATA BUCKET (Read-Only)\n",
        "DATA_BUCKET_NAME = \"hm-recommendation-workshop\"\n",
        "DATA_GCS_PATH = f\"gs://{DATA_BUCKET_NAME}\"\n",
        "\n",
        "# 2. PRIVATE WORK BUCKET (Write)\n",
        "WORK_BUCKET_NAME = f\"hm-workshop-{PROJECT_ID}\"\n",
        "WORK_GCS_PATH = f\"gs://{WORK_BUCKET_NAME}\"\n",
        "\n",
        "# ARTIFACTS PATH\n",
        "ARTIFACTS_PATH = os.path.join(WORK_GCS_PATH, 'models/ranking_model')\n",
        "\n",
        "# INPUT DATA PATHS\n",
        "ARTICLES_PATH = os.path.join(DATA_GCS_PATH, 'articles.csv')\n",
        "CUSTOMERS_PATH = os.path.join(DATA_GCS_PATH, 'customers.csv')\n",
        "TRANSACTIONS_PATH = os.path.join(DATA_GCS_PATH, 'transactions.csv')\n",
        "\n",
        "# TWO-TOWER MODEL PATH\n",
        "RETRIEVAL_MODEL_PATH = os.path.join(WORK_GCS_PATH, 'models/two-tower-model')\n",
        "\n",
        "# @markdown ### ðŸ§ª Experiment Settings\n",
        "TOP_K_RETRIEVAL = 60 # @param {type:\"integer\"}\n",
        "NUM_TRAIN_WEEKS = 6 # @param {type:\"slider\", min:1, max:6}\n",
        "LEARNING_RATE = 0.05 # @param {type:\"number\"}\n",
        "NUM_LEAVES = 63 # @param {type:\"integer\"}\n",
        "NUM_ROUNDS = 1000 # @param {type:\"integer\"}\n",
        "\n",
        "os.environ[\"GCLOUD_PROJECT\"] = PROJECT_ID\n",
        "\n",
        "print(f\"âœ… Config Set:\")\n",
        "print(f\"   ðŸ“¥ Raw Data: {DATA_GCS_PATH}\")\n",
        "print(f\"   ðŸ” Retrieval Model: {RETRIEVAL_MODEL_PATH}\")\n",
        "print(f\"   ðŸ’¾ Ranking Model Output: {ARTIFACTS_PATH}\")\n",
        "print(\"âš ï¸ Please run the next cell to install dependencies.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# @title ðŸ“¥ Step 1: Install Dependencies\n",
        "# @markdown Installing libraries using the standard method (similar to Retrieval notebook).\n",
        "\n",
        "# 1. Install TensorFlow Recommenders without dependencies first\n",
        "!pip install -q tensorflow-recommenders --no-deps\n",
        "\n",
        "# 2. Install ScaNN (TensorFlow compatible) and other libs\n",
        "!pip install -q \"scann[tf]\" tensorflow-recommenders lightgbm pandas numpy gcsfs\n",
        "\n",
        "print(\"âœ… Installation Complete.\")\n",
        "print(\"âš ï¸ Please RESTART RUNTIME (Runtime > Restart Runtime) before running the next cell.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# @title ðŸ“š Step 2: Import Libraries\n",
        "import gc\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "# Import order matters for registering ops\n",
        "import tensorflow as tf\n",
        "import tensorflow_recommenders as tfrs\n",
        "\n",
        "# --- IMPORTANT: SCANN IMPORT ---\n",
        "# Import ScaNN after TF to register C++ operations.\n",
        "import scann\n",
        "# -------------------------------\n",
        "\n",
        "def apk(actual, predicted, k=10):\n",
        "    if len(predicted) > k: predicted = predicted[:k]\n",
        "    score = 0.0\n",
        "    num_hits = 0.0\n",
        "    for i, p in enumerate(predicted):\n",
        "        if p in actual and p not in predicted[:i]:\n",
        "            num_hits += 1.0\n",
        "            score += num_hits / (i + 1.0)\n",
        "    if not actual: return 0.0\n",
        "    return score / min(len(actual), k)\n",
        "\n",
        "def mapk(actual, predicted, k=10):\n",
        "    return np.mean([apk(a, p, k) for a, p in zip(actual, predicted)])\n",
        "\n",
        "print(f\"TensorFlow Version: {tf.__version__}\")\n",
        "print(\"âœ… Libraries Imported.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# @title ðŸ’¾ Step 3: Load Static Data\n",
        "def load_static_data():\n",
        "    print(\">>> Loading Static Data...\")\n",
        "    cols = ['article_id', 'product_code', 'product_type_name', 'product_group_name',\n",
        "            'graphical_appearance_no', 'colour_group_code', 'section_no', 'garment_group_no']\n",
        "    articles = pd.read_csv(ARTICLES_PATH, dtype={'article_id': str}, usecols=cols)\n",
        "    for c in cols:\n",
        "        if c != 'article_id': articles[c] = pd.factorize(articles[c].astype(str), sort=True)[0]\n",
        "\n",
        "    articles['article_id_int'], _ = pd.factorize(articles['article_id'], sort=True)\n",
        "    article_map = dict(zip(articles['article_id'], articles['article_id_int']))\n",
        "    articles['article_id'] = articles['article_id_int']\n",
        "    del articles['article_id_int']\n",
        "\n",
        "    cust_cols = ['customer_id', 'FN', 'Active', 'age', 'club_member_status']\n",
        "    customers = pd.read_csv(CUSTOMERS_PATH, usecols=cust_cols, dtype={'customer_id': str})\n",
        "    customers['FN'] = customers['FN'].fillna(0)\n",
        "    customers['Active'] = customers['Active'].fillna(0)\n",
        "    customers['age'] = customers['age'].fillna(customers['age'].mean())\n",
        "    customers['club_member_status'] = pd.factorize(customers['club_member_status'].fillna('Unknown'), sort=True)[0]\n",
        "\n",
        "    customers['customer_id_int'], _ = pd.factorize(customers['customer_id'], sort=True)\n",
        "    customer_map = dict(zip(customers['customer_id'], customers['customer_id_int']))\n",
        "    customers['customer_id'] = customers['customer_id_int']\n",
        "    del customers['customer_id_int']\n",
        "\n",
        "    return articles, customers, article_map, customer_map\n",
        "\n",
        "articles_df, customers_df, article_map, customer_map = load_static_data()\n",
        "print(\"âœ… Static data loaded.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# @markdown Fixed: Accessing model output as tuple (scores, candidates) instead of dict.\n",
        "\n",
        "def generate_weekly_data(target_start_date, df_trans, tf_model, is_training=True):\n",
        "    history_cutoff = target_start_date\n",
        "    target_end_date = target_start_date + timedelta(days=7)\n",
        "    df_history = df_trans[df_trans['t_dat'] < history_cutoff]\n",
        "\n",
        "    if is_training:\n",
        "        df_target = df_trans[(df_trans['t_dat'] >= target_start_date) & (df_trans['t_dat'] < target_end_date)]\n",
        "        target_users = df_target['customer_id'].unique()\n",
        "    else:\n",
        "        df_target = df_trans[df_trans['t_dat'] >= target_start_date]\n",
        "        target_users = df_target['customer_id'].unique()\n",
        "\n",
        "    if len(target_users) == 0: return None\n",
        "\n",
        "    last_week_start = history_cutoff - timedelta(days=7)\n",
        "    df_last_week = df_history[df_history['t_dat'] > last_week_start]\n",
        "    item_trend_score = df_last_week.groupby('article_id').size().reset_index(name='trend_score')\n",
        "\n",
        "    hist_age = df_history[['article_id', 'customer_id']].merge(customers_df[['customer_id', 'age']], on='customer_id')\n",
        "    item_avg_age = hist_age.groupby('article_id')['age'].mean().reset_index(name='item_avg_age')\n",
        "\n",
        "    top_items = item_trend_score.sort_values('trend_score', ascending=False).head(12)['article_id'].tolist()\n",
        "\n",
        "    repurchase_start = history_cutoff - timedelta(days=28)\n",
        "    df_rep = df_history[(df_history['t_dat'] > repurchase_start) & (df_history['customer_id'].isin(target_users))]\n",
        "    user_history = df_rep.groupby('customer_id')['article_id'].apply(lambda x: list(set(x))).to_dict()\n",
        "\n",
        "    inv_cust_map = {v: k for k, v in customer_map.items()}\n",
        "    tf_cands_dict = {}\n",
        "    tf_scores_dict = {}\n",
        "\n",
        "    BATCH = 1000\n",
        "    tgt_list = list(target_users)\n",
        "\n",
        "    for i in range(0, len(tgt_list), BATCH):\n",
        "        batch_uids = tgt_list[i:i+BATCH]\n",
        "        batch_strs = [inv_cust_map[u] for u in batch_uids]\n",
        "        inp = {\n",
        "            \"customer_id\": tf.constant(batch_strs),\n",
        "            \"age_bin\": tf.constant([\"25\"]*len(batch_strs)),\n",
        "            \"month_of_year\": tf.constant([\"9\"]*len(batch_strs)),\n",
        "            \"week_of_month\": tf.constant([\"2\"]*len(batch_strs))\n",
        "        }\n",
        "        res = tf_model(inp)\n",
        "\n",
        "        # --- FIX HERE ---\n",
        "        # When SavedModel is loaded, ScaNN output is usually a tuple (scores, candidates).\n",
        "        # res[0] -> Scores\n",
        "        # res[1] -> Candidates (IDs)\n",
        "        if isinstance(res, dict):\n",
        "            cands = res['candidates'].numpy().astype(str)\n",
        "            scores = res['scores'].numpy()\n",
        "        else:\n",
        "            # Tuple behavior (SavedModel)\n",
        "            scores = res[0].numpy()\n",
        "            cands = res[1].numpy().astype(str)\n",
        "        # ------------------------\n",
        "\n",
        "        for idx, u in enumerate(batch_uids):\n",
        "            c_list = []\n",
        "            s_map = {}\n",
        "            for j in range(min(TOP_K_RETRIEVAL, len(cands[idx]))):\n",
        "                art_str = cands[idx][j]\n",
        "                if art_str in article_map:\n",
        "                    art_int = article_map[art_str]\n",
        "                    c_list.append(art_int)\n",
        "                    s_map[art_int] = float(scores[idx][j])\n",
        "            tf_cands_dict[u] = c_list\n",
        "            tf_scores_dict[u] = s_map\n",
        "\n",
        "    data = []\n",
        "    for u in target_users:\n",
        "        candidates = set()\n",
        "        candidates.update(top_items)\n",
        "        if u in user_history: candidates.update(user_history[u])\n",
        "        if u in tf_cands_dict: candidates.update(tf_cands_dict[u])\n",
        "\n",
        "        for aid in candidates:\n",
        "            t_score = tf_scores_dict.get(u, {}).get(aid, 0.0)\n",
        "            data.append([u, aid, t_score])\n",
        "\n",
        "    df_week = pd.DataFrame(data, columns=['customer_id', 'article_id', 'tf_score'])\n",
        "\n",
        "    if is_training:\n",
        "        df_target['purchased'] = 1\n",
        "        truth = df_target[['customer_id', 'article_id', 'purchased']].drop_duplicates()\n",
        "        df_week = df_week.merge(truth, on=['customer_id', 'article_id'], how='left')\n",
        "        df_week['label'] = df_week['purchased'].fillna(0).astype('int8')\n",
        "        del df_week['purchased']\n",
        "\n",
        "        pos = df_week[df_week['label'] == 1]\n",
        "        neg = df_week[df_week['label'] == 0]\n",
        "        if len(neg) > 1_500_000:\n",
        "            neg = neg.sample(n=1_500_000, random_state=42)\n",
        "        df_week = pd.concat([pos, neg])\n",
        "\n",
        "    df_week = df_week.merge(item_trend_score, on='article_id', how='left').fillna({'trend_score': 0})\n",
        "    df_week = df_week.merge(customers_df[['customer_id', 'age']], on='customer_id', how='left')\n",
        "    df_week = df_week.merge(item_avg_age, on='article_id', how='left')\n",
        "    df_week['item_avg_age'] = df_week['item_avg_age'].fillna(30)\n",
        "    df_week['age_diff'] = np.abs(df_week['age'] - df_week['item_avg_age'])\n",
        "    df_week = df_week.merge(articles_df, on='article_id', how='left')\n",
        "    cust_cols_static = [c for c in customers_df.columns if c not in ['age', 'customer_id']]\n",
        "    df_week = df_week.merge(customers_df[['customer_id'] + cust_cols_static], on='customer_id', how='left')\n",
        "\n",
        "    return df_week\n",
        "\n",
        "print(\"âœ… Data Generation Engine ready.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# @title ðŸ‹ï¸ Step 5: Train LightGBM Model\n",
        "print(\">>> Loading Transactions...\")\n",
        "df_trans = pd.read_csv(TRANSACTIONS_PATH, dtype={'article_id': str, 'customer_id': str}, parse_dates=['t_dat'])\n",
        "df_trans['article_id'] = df_trans['article_id'].map(article_map).fillna(-1).astype('int32')\n",
        "df_trans['customer_id'] = df_trans['customer_id'].map(customer_map).fillna(-1).astype('int32')\n",
        "df_trans = df_trans[(df_trans['article_id'] != -1) & (df_trans['customer_id'] != -1)]\n",
        "\n",
        "print(\">>> Loading Retrieval Model...\")\n",
        "\n",
        "# 1. Helper Function: Find saved_model.pb recursively\n",
        "def find_model_path(base_dir):\n",
        "    for root, dirs, files in os.walk(base_dir):\n",
        "        if \"saved_model.pb\" in files:\n",
        "            return root\n",
        "    return None\n",
        "\n",
        "# 2. Download Model\n",
        "if os.path.exists(\"two-tower-model\"):\n",
        "    os.system(\"rm -rf two-tower-model\")\n",
        "os.makedirs(\"two-tower-model\", exist_ok=True)\n",
        "\n",
        "# Debug Check\n",
        "if os.system(f\"gsutil -q stat {RETRIEVAL_MODEL_PATH}/saved_model.pb\") != 0:\n",
        "    print(f\"âŒ ERROR: File not found in GCS: {RETRIEVAL_MODEL_PATH}/saved_model.pb\")\n",
        "    raise FileNotFoundError(\"Retrieval model failed to download.\")\n",
        "\n",
        "print(f\"   Downloading from: {RETRIEVAL_MODEL_PATH}\")\n",
        "# Wildcard download\n",
        "os.system(f\"gsutil -m cp -r '{RETRIEVAL_MODEL_PATH}/*' two-tower-model/\")\n",
        "\n",
        "# 3. Locate & Load Model\n",
        "model_dir = find_model_path(\"two-tower-model\")\n",
        "\n",
        "if model_dir:\n",
        "    print(f\"âœ… Found SavedModel at: {model_dir}\")\n",
        "    import scann\n",
        "    tf_model = tf.saved_model.load(model_dir)\n",
        "else:\n",
        "    print(\"âŒ saved_model.pb not found after download.\")\n",
        "    raise FileNotFoundError(\"saved_model.pb not found.\")\n",
        "\n",
        "print(f\"Generating training data for {NUM_TRAIN_WEEKS} weeks...\")\n",
        "VAL_WEEK_START = pd.to_datetime('2020-09-16')\n",
        "\n",
        "big_train_df = pd.DataFrame()\n",
        "for w in range(1, NUM_TRAIN_WEEKS + 1):\n",
        "    target_start = VAL_WEEK_START - timedelta(weeks=w)\n",
        "    print(f\"   Processing Week: {target_start.date()}\")\n",
        "    week_df = generate_weekly_data(target_start, df_trans, tf_model, is_training=True)\n",
        "    if week_df is not None:\n",
        "        big_train_df = pd.concat([big_train_df, week_df])\n",
        "        del week_df\n",
        "        gc.collect()\n",
        "\n",
        "print(\"Preparing LightGBM Dataset...\")\n",
        "big_train_df = big_train_df.sort_values(by=['customer_id'], kind='mergesort')\n",
        "drop_cols = ['customer_id', 'article_id', 'label']\n",
        "X = big_train_df.drop(columns=drop_cols)\n",
        "y = big_train_df['label']\n",
        "group = big_train_df.groupby('customer_id', sort=False).size().to_numpy()\n",
        "\n",
        "cat_cols = ['product_code', 'product_type_name', 'product_group_name',\n",
        "            'graphical_appearance_no', 'colour_group_code', 'section_no',\n",
        "            'garment_group_no', 'club_member_status']\n",
        "\n",
        "train_set = lgb.Dataset(X, y, group=group, categorical_feature=cat_cols, free_raw_data=False)\n",
        "\n",
        "params = {\n",
        "    'objective': 'lambdarank',\n",
        "    'metric': 'map',\n",
        "    'eval_at': [12],\n",
        "    'learning_rate': LEARNING_RATE,\n",
        "    'num_leaves': NUM_LEAVES,\n",
        "    'max_depth': -1,\n",
        "    'feature_fraction': 0.7,\n",
        "    'bagging_fraction': 0.7,\n",
        "    'bagging_freq': 1,\n",
        "    'force_col_wise': True,\n",
        "    'verbose': 100\n",
        "}\n",
        "\n",
        "print(\">>> Starting Training...\")\n",
        "model = lgb.train(params, train_set, num_boost_round=NUM_ROUNDS)\n",
        "model.save_model(\"model.model\")\n",
        "\n",
        "print(\">>> Evaluating...\")\n",
        "eval_df = generate_weekly_data(VAL_WEEK_START, df_trans, tf_model, is_training=False)\n",
        "model_features = model.feature_name()\n",
        "for c in model_features:\n",
        "    if c not in eval_df.columns: eval_df[c] = 0\n",
        "\n",
        "X_eval = eval_df[model_features]\n",
        "eval_df['score'] = model.predict(X_eval)\n",
        "\n",
        "top_recs = eval_df.sort_values(['customer_id', 'score'], ascending=[True, False]).groupby('customer_id').head(12)\n",
        "preds_map = top_recs.groupby('customer_id')['article_id'].apply(list).to_dict()\n",
        "\n",
        "val_target_df = df_trans[df_trans['t_dat'] >= VAL_WEEK_START]\n",
        "ground_truth = val_target_df.groupby('customer_id')['article_id'].apply(list).to_dict()\n",
        "val_users = list(ground_truth.keys())\n",
        "\n",
        "map_scores = [apk(ground_truth[uid], preds_map[uid], k=12) if uid in preds_map else 0.0 for uid in val_users]\n",
        "final_map = np.mean(map_scores)\n",
        "print(f\"ðŸŽ‰ FINAL MAP@12 SCORE: {final_map:.5f}\")\n",
        "\n",
        "print(f\"Uploading model to {ARTIFACTS_PATH}...\")\n",
        "os.system(f\"gsutil cp model.model {ARTIFACTS_PATH}/model.model\")\n",
        "print(\"âœ… Training and Evaluation complete.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# @markdown This cell finds and reports the most successful predictions (hits) of the trained model on the validation set.\n",
        "\n",
        "print(\">>> Loading Product Names for Human-Readable Report...\")\n",
        "# Extracting names from the original file\n",
        "raw_articles = pd.read_csv(ARTICLES_PATH, usecols=['article_id', 'prod_name', 'product_type_name'], dtype={'article_id': str})\n",
        "# We need to convert IDs to int because model output is int\n",
        "raw_articles['article_id_int'] = raw_articles['article_id'].map(article_map).fillna(-1).astype(int)\n",
        "raw_articles = raw_articles[raw_articles['article_id_int'] != -1]\n",
        "\n",
        "# Map: Int ID -> \"Product Name (Type)\"\n",
        "name_map = dict(zip(raw_articles['article_id_int'], raw_articles['prod_name'] + ' (' + raw_articles['product_type_name'] + ')'))\n",
        "# Map: Int ID -> String ID\n",
        "inv_article_map = dict(zip(raw_articles['article_id_int'], raw_articles['article_id']))\n",
        "\n",
        "# Customer Map Inverse\n",
        "inv_cust_map = {v: k for k, v in customer_map.items()}\n",
        "\n",
        "print(\">>> Analyzing Hits...\")\n",
        "success_list = []\n",
        "\n",
        "# preds_map and ground_truth are available in memory from the previous cell!\n",
        "for uid, items in preds_map.items():\n",
        "    if uid in ground_truth:\n",
        "        actual = set(ground_truth[uid])\n",
        "        hits = list(set(items) & actual)\n",
        "\n",
        "        if len(hits) > 0:\n",
        "            # Convert to human readable format\n",
        "            u_str = inv_cust_map.get(uid, \"Unknown\")\n",
        "\n",
        "            # Hit details\n",
        "            hit_details = []\n",
        "            hit_ids = []\n",
        "            for h in hits:\n",
        "                h_str = inv_article_map.get(h, str(h))\n",
        "                h_name = name_map.get(h, \"Unknown Product\")\n",
        "                hit_details.append(f\"{h_name}\")\n",
        "                hit_ids.append(h_str)\n",
        "\n",
        "            success_list.append({\n",
        "                'customer_id': u_str,\n",
        "                'hit_count': len(hits),\n",
        "                'hit_names': \", \".join(hit_details),\n",
        "                'hit_ids': \", \".join(hit_ids)\n",
        "            })\n",
        "\n",
        "df_success = pd.DataFrame(success_list).sort_values('hit_count', ascending=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"DEMO REPORT: At least 1 hit for a total of {len(df_success)} users!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if not df_success.empty:\n",
        "    print(\"\\nðŸ† TOP 10 PREDICTION EXAMPLES:\")\n",
        "    for i, row in df_success.head(10).iterrows():\n",
        "        print(f\"ðŸ‘¤ Customer: {row['customer_id']}\")\n",
        "        print(f\"ðŸ† Hit Count: {row['hit_count']}\")\n",
        "        print(f\"âœ… Known Products: {row['hit_names']} (ID: {row['hit_ids']})\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "    # Save CSV\n",
        "    df_success.to_csv('demo_success_results.csv', index=False)\n",
        "    print(f\"ðŸ“„ Detailed report saved: demo_success_results.csv\")\n",
        "else:\n",
        "    print(\"ðŸ˜” Unfortunately, there are no hits in this validation set.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# @title ðŸ“¦ Step 6: Prepare & Upload Serving Artifacts (FULL & FIXED)\n",
        "import pickle\n",
        "import gc\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# We fix everything here to prevent ID Mismatch.\n",
        "print(\">>> 1. Saving Mapping Objects (Dictionary)...\")\n",
        "\n",
        "# article_map: \"0108775015\" (str) -> 5 (int)\n",
        "with open('article_map.pkl', 'wb') as f:\n",
        "    pickle.dump(article_map, f)\n",
        "\n",
        "# customer_map: \"000058a12d...\" (str) -> 2 (int)\n",
        "with open('customer_map.pkl', 'wb') as f:\n",
        "    pickle.dump(customer_map, f)\n",
        "\n",
        "# Inverse Mapping (Int -> Str) - Will be needed shortly\n",
        "inv_article_map = {v: k for k, v in article_map.items()}\n",
        "inv_customer_map = {v: k for k, v in customer_map.items()}\n",
        "\n",
        "print(\">>> 2. Processing Articles for Serving (Feature Store)...\")\n",
        "# articles_df is already in memory with Integer IDs (from Step 3).\n",
        "# Let's add the original String ID as a column just in case.\n",
        "serving_articles = articles_df.copy()\n",
        "serving_articles['article_id_str'] = serving_articles['article_id'].map(inv_article_map)\n",
        "\n",
        "# Optimize feature data types (Category)\n",
        "for col in serving_articles.columns:\n",
        "    if serving_articles[col].dtype == 'object' and col != 'article_id_str':\n",
        "        serving_articles[col] = serving_articles[col].astype('category')\n",
        "\n",
        "serving_articles.to_parquet('app_articles_features.parquet', index=False)\n",
        "print(\"   -> app_articles_features.parquet saved.\")\n",
        "\n",
        "print(\">>> 3. Processing Customers for Serving...\")\n",
        "# customers_df is also in memory with Integer IDs.\n",
        "serving_customers = customers_df.copy()\n",
        "serving_customers['customer_id_str'] = serving_customers['customer_id'].map(inv_customer_map)\n",
        "serving_customers.to_parquet('app_customers_features.parquet', index=False)\n",
        "print(\"   -> app_customers_features.parquet saved.\")\n",
        "\n",
        "print(\">>> 4. Processing User History (Integer IDs)...\")\n",
        "# df_trans was already converted to Integer IDs in Step 5.\n",
        "# It is best to store history as Integers since the model expects Integer IDs.\n",
        "VAL_START = pd.to_datetime('2020-09-16')\n",
        "hist_start = VAL_START - timedelta(days=28)\n",
        "\n",
        "# Data for the last 28 days\n",
        "df_hist = df_trans[(df_trans['t_dat'] >= hist_start) & (df_trans['t_dat'] < VAL_START)]\n",
        "\n",
        "# List of items purchased for each user (Integer List)\n",
        "user_history = df_hist.groupby('customer_id')['article_id'].apply(list).reset_index()\n",
        "user_history.columns = ['customer_id', 'article_ids'] # customer_id: INT, article_ids: LIST[INT]\n",
        "\n",
        "user_history.to_parquet('app_user_history_int.parquet', index=False)\n",
        "print(\"   -> app_user_history_int.parquet saved (Integer IDs).\")\n",
        "\n",
        "print(\">>> 5. Generating Stats / Trending Items (Integer IDs)...\")\n",
        "# Most popular items of the last week (for Cold Start and Candidate recommendation)\n",
        "last_week_start = VAL_START - timedelta(days=7)\n",
        "df_trend = df_trans[(df_trans['t_dat'] >= last_week_start) & (df_trans['t_dat'] < VAL_START)]\n",
        "\n",
        "# Count and save\n",
        "item_stats = df_trend.groupby('article_id').size().reset_index(name='trend_score')\n",
        "item_stats['item_avg_age'] = 30.0 # Simplified average age (calculate real if needed)\n",
        "\n",
        "# article_id in item_stats is already Integer.\n",
        "item_stats.to_parquet('app_stats_int.parquet', index=False)\n",
        "print(\"   -> app_stats_int.parquet saved (Integer IDs).\")\n",
        "\n",
        "print(\">>> 6. Processing Validation Truth (STRING IDs for Frontend)...\")\n",
        "# We need String IDs to show \"What did they actually buy?\" on the Streamlit side.\n",
        "# Because we don't want to load pickle maps on the frontend to keep file size small.\n",
        "val_df = df_trans[df_trans['t_dat'] >= VAL_START].copy()\n",
        "\n",
        "# Converting Integer IDs back to String\n",
        "val_df['article_id'] = val_df['article_id'].map(inv_article_map)\n",
        "val_df['customer_id'] = val_df['customer_id'].map(inv_customer_map)\n",
        "\n",
        "# Clean if there are NaNs (Unmapped)\n",
        "val_df = val_df.dropna(subset=['article_id', 'customer_id'])\n",
        "\n",
        "# Save only necessary columns\n",
        "val_df[['customer_id', 'article_id']].to_parquet('val_truth.parquet', index=False)\n",
        "print(\"   -> val_truth.parquet saved (String IDs).\")\n",
        "\n",
        "print(f\"\\nðŸš€ Uploading artifacts to {ARTIFACTS_PATH}...\")\n",
        "upload_files = [\n",
        "    'article_map.pkl',\n",
        "    'customer_map.pkl',\n",
        "    'app_articles_features.parquet',\n",
        "    'app_customers_features.parquet',\n",
        "    'app_user_history_int.parquet',\n",
        "    'app_stats_int.parquet',\n",
        "    'val_truth.parquet',\n",
        "    'model.model' # Let's secure the LightGBM model as well\n",
        "]\n",
        "\n",
        "for f in upload_files:\n",
        "    if os.path.exists(f):\n",
        "        os.system(f\"gsutil cp {f} {ARTIFACTS_PATH}/\")\n",
        "        print(f\"   -> Uploaded: {f}\")\n",
        "    else:\n",
        "        print(f\"   âš ï¸ Warning: File not found {f}\")\n",
        "\n",
        "print(\"\\nâœ… All Serving Artifacts are Ready & Consistent!\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# @title ðŸ“¦ Step 7: Create Backend Application (FIXED)\n",
        "\n",
        "import os\n",
        "\n",
        "os.makedirs(\"deploy_app\", exist_ok=True)\n",
        "\n",
        "# Writing main.py with updated file names and import fixes\n",
        "app_code = f\"\"\"\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "import tensorflow as tf\n",
        "import lightgbm as lgb\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import gc\n",
        "import contextlib\n",
        "import traceback\n",
        "\n",
        "# --- CONFIG ---\n",
        "BUCKET_NAME = '{WORK_BUCKET_NAME}'\n",
        "GCS_BASE = f'gs://{{BUCKET_NAME}}'\n",
        "ARTIFACTS_PATH = f'{{GCS_BASE}}/models/ranking_model'\n",
        "TF_PATH = f'{{GCS_BASE}}/models/two-tower-model'\n",
        "\n",
        "models = {{}}\n",
        "data = {{}}\n",
        "TOP_K_TREND = 12\n",
        "\n",
        "def generate_image_url(article_id_str):\n",
        "    # Ensure ID is string and 10 digits (for the leading zero)\n",
        "    aid_str = str(article_id_str).strip().zfill(10)\n",
        "    folder = aid_str[:3]\n",
        "    return f\"https://repo.hops.works/dev/jdowling/h-and-m/images/{{folder}}/{{aid_str}}.jpg\"\n",
        "\n",
        "@contextlib.asynccontextmanager\n",
        "async def lifespan(app: FastAPI):\n",
        "    print(\">>> [INIT] Loading Artifacts...\")\n",
        "    try:\n",
        "        # 1. Download Files\n",
        "        files = [\n",
        "            'article_map.pkl',\n",
        "            'customer_map.pkl',\n",
        "            'app_articles_features.parquet',\n",
        "            'app_customers_features.parquet',\n",
        "            'app_stats_int.parquet',\n",
        "            'app_user_history_int.parquet',\n",
        "            'model.model'\n",
        "        ]\n",
        "\n",
        "        for f in files:\n",
        "            if not os.path.exists(f):\n",
        "                print(f\"Downloading {{f}}...\")\n",
        "                os.system(f\"gsutil cp {{ARTIFACTS_PATH}}/{{f}} .\")\n",
        "\n",
        "        if not os.path.exists(\"two-tower-model\"):\n",
        "            os.makedirs(\"two-tower-model\", exist_ok=True)\n",
        "            os.system(f\"gsutil -m cp -r {{TF_PATH}}/* two-tower-model/\")\n",
        "\n",
        "        # 2. Load MAPs (Critical Step)\n",
        "        print(\">>> Loading Maps...\")\n",
        "        with open('article_map.pkl', 'rb') as f:\n",
        "            article_map = pickle.load(f) # String -> Int\n",
        "\n",
        "        with open('customer_map.pkl', 'rb') as f:\n",
        "            customer_map = pickle.load(f) # String -> Int\n",
        "\n",
        "        # Inverse Map (Int -> String)\n",
        "        inv_article_map = {{v: k for k, v in article_map.items()}}\n",
        "\n",
        "        # 3. Load Feature Tables (Already with Integer IDs)\n",
        "        print(\">>> Loading Features...\")\n",
        "        articles_feat = pd.read_parquet('app_articles_features.parquet')\n",
        "        articles_feat = articles_feat.set_index('article_id')\n",
        "\n",
        "        customers_feat = pd.read_parquet('app_customers_features.parquet')\n",
        "        customers_feat = customers_feat.set_index('customer_id')\n",
        "\n",
        "        stats = pd.read_parquet('app_stats_int.parquet')\n",
        "        top_trend_ids = stats.sort_values('trend_score', ascending=False).head(TOP_K_TREND)['article_id'].tolist()\n",
        "\n",
        "        # User History (List of Integers)\n",
        "        user_history_map = {{}}\n",
        "        if os.path.exists('app_user_history_int.parquet'):\n",
        "            hist_df = pd.read_parquet('app_user_history_int.parquet')\n",
        "            # customer_id (int) -> article_ids (list of int)\n",
        "            user_history_map = dict(zip(hist_df['customer_id'], hist_df['article_ids']))\n",
        "\n",
        "        # Register to Global Data\n",
        "        data['article_map'] = article_map\n",
        "        data['inv_article_map'] = inv_article_map\n",
        "        data['customer_map'] = customer_map\n",
        "        data['articles_feat'] = articles_feat\n",
        "        data['customers_feat'] = customers_feat\n",
        "        data['stats'] = stats.set_index('article_id')\n",
        "        data['top_trend_ids'] = top_trend_ids\n",
        "        data['user_history_map'] = user_history_map\n",
        "\n",
        "        # 4. Load Models\n",
        "        print(\">>> Loading Models...\")\n",
        "        models['lgb'] = lgb.Booster(model_file='model.model')\n",
        "\n",
        "        try:\n",
        "            # TF ScaNN import sometimes requires a trick\n",
        "            import scann\n",
        "            models['tf'] = tf.saved_model.load(\"two-tower-model\")\n",
        "            print(\"   -> TF/ScaNN Loaded.\")\n",
        "        except Exception as e:\n",
        "            print(f\"   -> TF Load Warning: {{e}}\")\n",
        "\n",
        "        print(\">>> [READY] Service loaded correctly.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"!!! Server Start Error: {{e}}\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "    yield\n",
        "    models.clear()\n",
        "    data.clear()\n",
        "    gc.collect()\n",
        "\n",
        "app = FastAPI(lifespan=lifespan)\n",
        "\n",
        "class RecRequest(BaseModel):\n",
        "    user_id: str\n",
        "    history: list[str] = []\n",
        "\n",
        "@app.post(\"/predict\")\n",
        "async def predict(req: RecRequest):\n",
        "    try:\n",
        "        uid_str = str(req.user_id)\n",
        "\n",
        "        # 1. Customer ID Conversion (String -> Int)\n",
        "        u_idx = data['customer_map'].get(uid_str, -1)\n",
        "\n",
        "        # 2. Candidate Generation - ALL INTEGERS\n",
        "        candidate_ids_int = set()\n",
        "\n",
        "        # A) Trends\n",
        "        candidate_ids_int.update(data['top_trend_ids'])\n",
        "\n",
        "        # B) Past Purchases (User History)\n",
        "        if u_idx != -1 and u_idx in data['user_history_map']:\n",
        "            # Take last 12 to prevent candidate explosion\n",
        "            past_items = data['user_history_map'][u_idx][-12:]\n",
        "            candidate_ids_int.update(past_items)\n",
        "\n",
        "        # C) Two-Tower (TF ScaNN)\n",
        "        if 'tf' in models:\n",
        "            try:\n",
        "                inp = {{\n",
        "                    \"customer_id\": tf.constant([uid_str]),\n",
        "                    \"age_bin\": tf.constant([\"25\"]),\n",
        "                    \"month_of_year\": tf.constant([\"9\"]),\n",
        "                    \"week_of_month\": tf.constant([\"2\"])\n",
        "                }}\n",
        "                res = models['tf'](inp)\n",
        "\n",
        "                # ScaNN outputs are String IDs.\n",
        "                if isinstance(res, dict): cands_str = res['candidates'].numpy()[0].astype(str)\n",
        "                else: cands_str = res[1].numpy()[0].astype(str)\n",
        "\n",
        "                # Convert String candidates to Integer\n",
        "                for c in cands_str:\n",
        "                    if c in data['article_map']:\n",
        "                        candidate_ids_int.add(data['article_map'][c])\n",
        "            except: pass\n",
        "\n",
        "        # D) Frontend Cart (History)\n",
        "        for h in req.history:\n",
        "            if h in data['article_map']:\n",
        "                candidate_ids_int.add(data['article_map'][h])\n",
        "\n",
        "        if not candidate_ids_int: return {{\"recommendations\": []}}\n",
        "\n",
        "        # 3. Create Feature Table\n",
        "        cand_list = list(candidate_ids_int)\n",
        "        df_cand = pd.DataFrame({{'article_id': cand_list}}) # article_id = INT\n",
        "\n",
        "        # Join Article Features\n",
        "        df_cand = df_cand.join(data['articles_feat'], on='article_id', rsuffix='_feat')\n",
        "\n",
        "        # Add Customer Features\n",
        "        if u_idx != -1 and u_idx in data['customers_feat'].index:\n",
        "            cust_row = data['customers_feat'].loc[u_idx]\n",
        "            for col in data['customers_feat'].columns:\n",
        "                if col != 'customer_id_str':\n",
        "                    df_cand[col] = cust_row[col]\n",
        "        else:\n",
        "            # Default values (Cold User)\n",
        "            df_cand['age'] = 30.0\n",
        "            df_cand['club_member_status'] = 0\n",
        "\n",
        "        # Add Stats (Trend Score)\n",
        "        if not data['stats'].empty:\n",
        "            df_cand = df_cand.join(data['stats'][['trend_score', 'item_avg_age']], on='article_id', rsuffix='_stat')\n",
        "\n",
        "        df_cand['trend_score'] = df_cand.get('trend_score', 0).fillna(0)\n",
        "        item_age = df_cand.get('item_avg_age', 30.0).fillna(30.0)\n",
        "\n",
        "        # Age Diff\n",
        "        df_cand['age_diff'] = np.abs(df_cand['age'] - item_age)\n",
        "\n",
        "        # 4. LightGBM Prediction\n",
        "        if 'lgb' in models:\n",
        "            feats = models['lgb'].feature_name()\n",
        "            # Ensure feature order\n",
        "            for f in feats:\n",
        "                if f not in df_cand.columns: df_cand[f] = 0\n",
        "\n",
        "            df_cand['score'] = models['lgb'].predict(df_cand[feats])\n",
        "        else:\n",
        "            df_cand['score'] = df_cand['trend_score'] # Fallback\n",
        "\n",
        "        # 5. Ranking and Result\n",
        "        top_recs = df_cand.sort_values('score', ascending=False).head(12)\n",
        "\n",
        "        results = []\n",
        "        for idx, row in top_recs.iterrows():\n",
        "            aid_int = int(row['article_id'])\n",
        "            # Int -> String ID\n",
        "            aid_str = data['inv_article_map'].get(aid_int, \"Unknown\")\n",
        "\n",
        "            results.append({{\n",
        "                \"article_id\": aid_str,\n",
        "                \"score\": float(row['score']),\n",
        "                \"image_url\": generate_image_url(aid_str),\n",
        "                \"prod_name\": str(row.get('prod_name', 'Product'))\n",
        "            }})\n",
        "\n",
        "        return {{\"recommendations\": results}}\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"!!! Predict Error: {{e}}\")\n",
        "        traceback.print_exc()\n",
        "        return {{\"error\": str(e), \"recommendations\": []}}\n",
        "\"\"\"\n",
        "\n",
        "with open(\"deploy_app/main.py\", \"w\") as f:\n",
        "    f.write(app_code)\n",
        "\n",
        "print(\"âœ… Backend code (main.py) updated with 'contextlib' fix.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 3. Write Dockerfile\n",
        "dockerfile_code = \"\"\"\n",
        "FROM gcr.io/google.com/cloudsdktool/google-cloud-cli:slim\n",
        "\n",
        "RUN apt-get update && apt-get install -y python3-pip python3-dev libgomp1 && \\\\\n",
        "    ln -s /usr/bin/python3 /usr/bin/python\n",
        "\n",
        "WORKDIR /app\n",
        "\n",
        "# TF 2.17 compatible\n",
        "RUN pip3 install --no-cache-dir --break-system-packages \\\\\n",
        "    flask gunicorn fastapi uvicorn \\\\\n",
        "    \"scann[tf]\" \\\\\n",
        "    tensorflow-recommenders \\\\\n",
        "    lightgbm pandas pyarrow gcsfs\n",
        "\n",
        "COPY main.py .\n",
        "\n",
        "CMD exec uvicorn main:app --host 0.0.0.0 --port 8080\n",
        "\"\"\"\n",
        "\n",
        "with open(\"deploy_app/Dockerfile\", \"w\") as f:\n",
        "    f.write(dockerfile_code)\n",
        "\n",
        "print(\"âœ… Deployment files generated.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 4. Build and Deploy\n",
        "IMAGE_NAME = f\"gcr.io/{PROJECT_ID}/hm-recommender-app\"\n",
        "SERVICE_NAME = \"hm-recommender-service\"\n",
        "\n",
        "# Build\n",
        "print(f\"ðŸ”¨ Building Container: {IMAGE_NAME}\")\n",
        "!gcloud builds submit --tag $IMAGE_NAME deploy_app\n",
        "\n",
        "print(f\"ðŸš€ Deploying to Cloud Run: {SERVICE_NAME}\")\n",
        "\n",
        "# TIMEOUT: 3600\n",
        "# MEMORY: 16Gi\n",
        "# CPU: 4\n",
        "!gcloud run deploy $SERVICE_NAME --image $IMAGE_NAME --platform managed --region $REGION --allow-unauthenticated --memory 16Gi --cpu 4 --timeout 3600 --cpu-boost\n",
        "\n",
        "print(\"âœ… Deployment Complete! Check the URL.\")"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}